{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut8_RNN_NLP2_student.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8: Text classification considering words as sequence\n",
    "In [tutorial 5](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut5_embeddings_teacher.ipynb), we saw how to classify the reviews in `IMBD` dataset into positive and negative sentiments. However, the approach didn't consider the order of the words in the review. Therefore, in this tutorial, we consider the sequence model approach rather than following the 'bag-of-words' model. \n",
    "\n",
    "For this purpose, we cover\n",
    "1. Load and prepare the well-known IMBD dataset for the sequence model approach.\n",
    "2. Masking, a way to tell RNNs to skip meaningless inputs\n",
    "3. Bidirectional RNNs\n",
    "4. GRU RNN \n",
    "\n",
    "For further examples, please visit [demos/nlp/sentiment_analysis.ipynb](https://github.com/Humboldt-WI/adams/blob/master/demos/nlp/sentiment_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess IMDB data for the sequence model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# load the data (be sure to provide the correct file path)\n",
    "total_imbd = pd.read_csv(\"../../../demos/nlp/IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "total_imbd['sentiment'] = total_imbd['sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(total_imbd['review'], total_imbd['sentiment'], test_size = 0.2, random_state = 5)\n",
    "# transform them to numpy \n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "# define standarization function \n",
    "def our_standardization(text_data):\n",
    "  lowercase = tf.strings.lower(text_data) # convert to lowercase\n",
    "  remove_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # remove HTML tags\n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  remove_punct = tf.strings.regex_replace(remove_html, pattern_remove_punctuation, '') # apply pattern\n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  return remove_double_spaces\n",
    "\n",
    "# Define the size of the vocabulary and the max number of words in a sequence\n",
    "vocab_size = 10000\n",
    "seq_length = 500\n",
    "\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "vectorize_layer.adapt(X_train)\n",
    "## Transform sequences of words to seq of integers and labels to tensor\n",
    "X_train = vectorize_layer(X_train)\n",
    "X_val = vectorize_layer(X_val)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking, a way to tell RNNs to skip meaningless inputs (padding)\n",
    "If our input sequences are full of zeros, that will hurt the model's performance. In our case, we have lots of zeros because we're using the `output_sequence_length=seq_length` option in `TextVectorization`. That truncates sentences longer than `seq_length` tokens to `seq_length` tokens but also pads shorter sentences with zeros.\n",
    "\n",
    "The RNN may spend its last iterations only seeing vectors encoding these zeros (short sentences). The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these empty inputs. To avoid this, we use masking. The `Embedding` layer can generate a mask (`mask_zero=True`) corresponding to its input data. This mask tells the RNN (as attached metadata) to skip over the iterations containing only vectors that encode padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "ex_emb = layers.Embedding(input_dim = 100, output_dim=16, mask_zero=True)\n",
    "ex_input = [[5,4,3,2,1,0,0],\n",
    "            [1,2,3,0,0,0,0]]\n",
    "ex_emb.compute_mask(ex_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Create a text classification model with `Embedding`+`LSTM`. Use `emb_size = 32` and `16` units for the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Fit the model using only 2 epochs and a `batch_size = 128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Predict the sentiment of the following phrases\n",
    "\n",
    "`\"This movie never stops surprising me. The actors are good.\"`\n",
    "\n",
    "`\"This movie never stops surprising me. The actors are good. However, the story is terrible.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "You have seen that RNNs care about the order (that's why they do well when the sequence order is essential). A bidirectional RNN is a type of recurrent neural network that is trained on two separate data sequences, one in chronological order and the other in reverse order. By doing so, we can learn patterns in both directions. \n",
    "\n",
    "If the sequence is formed by words, extracting patterns in both directions makes sense since, a priori, the potential relevance of a word in understanding a phrase is not entirely dependent on its position (the order is determined by the grammar rather than the sequential occurrence).  \n",
    "\n",
    "| ![](https://www.gabormelli.com/RKB/images/4/4f/BRNN_Mike_Paliwal_1997_Fig3.png) | \n",
    "|:--:| \n",
    "| (Schuster & Paliwal, 1997) |\n",
    "\n",
    "You can use `Bidirectional` layer in Keras to create a bidirectional RNN.\n",
    "\n",
    "```python\n",
    "inputs = tf.keras.Input(shape=(sequence_length, ))\n",
    "x = layers.Bidirectional(layers.LSTM(n_units))(inputs)\n",
    " ...\n",
    " ...\n",
    " ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Create a new model similar to the previous one but using a `Bidirectional` layer instead of one `LSTM`. Check the number of parameters in the new layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Fit the model with the same arguments as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Predict the sentiment for the previous sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"This movie never stops surprising me. The actors are good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"This movie never stops surprising me. The actors are good. However, the story is terrible.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with another RNN\n",
    "GRU stands for \"Gated Recurrent Unit\". GRUs are similar to LSTM, but they are more straightforward and were introduced more recently by [Cho et al. (2014)](https://arxiv.org/abs/1409.1259).\n",
    "\n",
    "![](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Using the `GRU` layer instead of the `LSTM`, create the text classification model and compare if there is a significant gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('adams')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
