{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a541085",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut4_NLP_pipeline_teacher.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7e7af",
   "metadata": {},
   "source": [
    "# Manual Training Loop with tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4357cf5",
   "metadata": {},
   "source": [
    "So far we implemeneted (simplified version) of backpropagation in the 2. tutorial with a single weight, and a set of several weights (not weights matrices). Manually implementing backpropagation gives you full controll over the training process. However, the more complicated the architecture of the network is, the more difficult it would be to manually implement backprop. For this purpose, in the 3. tutorial we used the high-level api of keras, which is very simple and user-friendly: with several lines of code we trained and tuned our neural network. On the negative side, when using the high-level api, you have very little control over what is happening during the training process. Therefore, in this tutorial we will implement a manual training loop using tensorflow, which gives you more felxibility than the high-level api of keras and is not as code-intensive as manually implementing backprop. One of the advantages of building a manual training loop is that , among others, you can examine the flow of gradients during the training process. Keras provides an example implementation of such training loop: https://keras.io/guides/writing_a_training_loop_from_scratch/. Following the example online, and using the boston housing dataset from 3. tutorial or a dataset that you have simulated, implement manually the training loop. For simplicity reasons, you could use data consisting of single batch during the training process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5be27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "# this will split the data 80-20% by default\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6727c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb90b5a8",
   "metadata": {},
   "source": [
    "# NLP pipeline\n",
    "You are already familiar with building predictive models on tabular data. In tabular data, you have a feature matrix `X` and a target vector `Y`. Given these data structures, you can apply learning algorithms to learn the relationship between `X` and `Y`. In this exercise, you are provided with a data set of movie reviews. Your goal is to build a classifier predicting whether a review has a positive or negative tone (this task is called sentiment classification). Hence, you have a prediction problem with a binary target, `Y`, which is nothing new for you. However, in this exercise, what is new for you is that you need to deal with text data instead of tabular data. With text data, you need to process the data to obtain the required feature matrix `X`. This processing of data is what we call the \"NLP pipeline\". \n",
    "\n",
    "In this exercise, you will need to set up an NLP pipeline. You are provided with a data set of movie reviews, where each sample contains a review (just a string cell). To obtain a feature matrix, each sample string cell needs to be transformed into a feature vector $x$. This process is called vectorization. There are multiple possible vectorization procedures. Today, you will implement a bag-of-words model for feature extraction. This feature extraction process involves two steps:\n",
    "1. Vocabulary building\n",
    " * Tokenization: Transforming a review, which is a single string at the beginning, into a vector of strings (tokens).\n",
    " * Cleaning and compressing techniques: Reducing the number of distinct tokens. E.g., correcting the misspelling of words or lower casing the letters prevents the same word from appearing in multiple spelling ways. Additionally, similar words (e.g. different forms of a verb) can be united into a single token. \n",
    " * Building a bag-of-words: a vector whose length corresponds exactly to the number of different tokens. Each token is assigned the position within the vector. \n",
    " \n",
    "2. Feature creation based on term frequency: Each review gets transformed into a feature vector $x$. The length of the feature vector corresponds to the length of the bag-of-words vector, created in step 1. An element $x_{j}$ of the feature vector is calculated by a frequency measure, measuring how frequently token $j$ from the bag-of-words vector occurs in the review. \n",
    "\n",
    "The first code cells provide the required packages and load the review data set, which you will use for the exercise. You will build the most simple NLP pipeline, which means that you go through steps 1 and 2 of the NLP pipeline, but you skip the \"cleaning and compressing\" part of step 1. This simple NLP pipeline provides you with a feature matrix `X` (possibly not ideal). You will use this feature matrix to build and evaluate a predictive model.\n",
    "\n",
    "In the tutorial, we will extend your NLP pipeline by including the cleaning and compressing techniques (according techniques are also covered in detail in the demo notebook `nlp_foundations.ipynb`). That will lead to another feature matrix, `X`. Then, we will build another predictive model on this new feature matrix `X` and compare the performance to the model built by the simplified NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec2f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('punkt') If needed\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup ## handles html\n",
    "import re ## provides regular expressions functionality\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6885cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remeber to adjust the path so that it matches your environment\n",
    "df = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5424923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get to know the data\n",
    "print(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2590c955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    5028\n",
       "negative    4973\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only use the first 10000 observations to reduce run time.\n",
    "df = df.loc[0:10000,:]\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)  # dropping the index prohibits a reidentification of the cases in the original data frame\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d35b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Give me a break. How can anyone say that this ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This movie is a bad movie. But after watching ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>This is a movie that was probably made to ente...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Smashing film about film-making. Shows the int...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>\" While sporadically engrossing (including a f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10001 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "9996   Give me a break. How can anyone say that this ...  negative\n",
       "9997   This movie is a bad movie. But after watching ...  negative\n",
       "9998   This is a movie that was probably made to ente...  negative\n",
       "9999   Smashing film about film-making. Shows the int...  positive\n",
       "10000  \" While sporadically engrossing (including a f...  negative\n",
       "\n",
       "[10001 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4fe5cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map label\n",
    "df['sentiment'] = df['sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198df3",
   "metadata": {},
   "source": [
    "## Simple NLP pipeline:\n",
    "You need to transform the text data, contained in the column `df[\"review\"]`, such that it is suitable as a feature matrix `X`, which you need for predictive model building. This means in detail: \n",
    "\n",
    "a) Create a list \"reviews_tokenized\", where each element corresponds to a string vector, representing a review. Use NLTK's `word_tokenize()` function. In case you get error messages from nltk, follow the instructions from the error to resolve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c499afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcfd67a6",
   "metadata": {},
   "source": [
    "b) Split the review data (`reviews_tokenized`) as well as the target `df['sentiment']` in training and test sets. Use 80% of the data for training. Use sklearn's `train_test_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b395c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11968263",
   "metadata": {},
   "source": [
    "c) Now, we need to set up a vocabulary for all tokens and apply this vocabulary to obtain feature vectors $x$. We do this using sklearn's `TfidfVectorizer`. Feel free to experiment with other feature engineering methods from the lecture, f.e., term frequency matrix (in sklearn `CountVectorizer`). You need to apply the vectorizer to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a33159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc  \n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor = dummy_fun,\n",
    "    token_pattern = None)\n",
    "\n",
    "## Set up the dictionary and calculate the document frequency of each token on the training set.\n",
    "## Then generate the features on the training set, using the document frequency table.\n",
    "\n",
    "## Apply the document frequency table on the test set, to generate feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14705493",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` did multiple steps at once. To better understand how it works, you should examine the results step by step.\n",
    "\n",
    "d) Examine the vocabulary it created: How many tokens does it include? Which tokens are included? Would it maybe be better to leave some of these tokens out to reduce the dimension of the vocabulary and the derived feature matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daa916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26d6fb5",
   "metadata": {},
   "source": [
    "e) Let's recap how feature vectors are generated from this vocabulary. The basic idea of bag-of-words based feature extraction is to generate for each token in the vocabulary a column in the feature matrix `X`. For an observation $i$ (corresponding to a single review), the entry $X_{i,j}$ of the feature matrix would be 1 if the review contains the token of column $j$ and 0 otherwise. There are some variations to this approach, f.e., the Tfidf approach (*term frequency-inverse document frequency*), which we apply in this exercise. Have a look at the matrix, which the `TfidfVectorizer` created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9972e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf162233",
   "metadata": {},
   "source": [
    "f) Fit a logistic regression classifier and evaluate the accuracy of the predictions on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50877e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2355b726",
   "metadata": {},
   "source": [
    "## Extended NLP Excercise\n",
    "In the previous exercise, we took little care about the cleaning and compressing part of the NLP pipeline. As a consequence, we obtained a dictionary with a lot of tokens which are most likely not so informative. The high number of tokens in the dictionary resulted in a very high dimension of the feature matrix `X`. In this exercise, we will add the cleaning and compressing part to the NLP pipeline. We hope to create a feature matrix of lower dimension, which yields more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60528c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download pre-learned NLP tools\n",
    "nltk.download('stopwords') ## to identify stopwords \n",
    "nltk.download('averaged_perceptron_tagger') ## for part-of-speech tagging (used for lemmatization)\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize with POS Tag (Parts of Speech tagging)\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character for lemmatization\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "## function to clean text data\n",
    "def clean_reviews(df):\n",
    "    \"\"\" Standard NLP pre-processing chain including removal of html tags, non-alphanumeric characters, and stopwords.\n",
    "        Words are subject to lemmatization using their POS tags, which are determind using WordNet. \n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print('*' * 40)\n",
    "    print('Cleaning {} movie reviews.'.format(df.shape[0]))\n",
    "    counter = 0\n",
    "    for review in df:\n",
    "        \n",
    "        # remove html content\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        \n",
    "        # remove non-alphabetic characters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "        # tokenize the sentences with all capital letters transformed to lower case\n",
    "        words = word_tokenize(review_text.lower())\n",
    "  \n",
    "        # filter stopwords\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words =[lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in words]\n",
    "    \n",
    "        reviews.append(lemma_words)\n",
    "              \n",
    "        if (counter > 0 and counter % 500 == 0):\n",
    "            print('Processed {} reviews'.format(counter))\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    print('DONE')\n",
    "    print('*' * 40)\n",
    "\n",
    "    return(reviews) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78648e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Do the cleaning\n",
    "# CAUTION: takes around 20 minutes \n",
    "reviews_clean = clean_reviews(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "225c8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(reviews_clean,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48399023",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load cleaned data\n",
    "with open('clean_reviews.pkl', 'rb') as f:\n",
    "    reviews_clean=pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9b965a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "n\n",
      "run\n",
      "runner\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "## While the text gets cleaned, we have a look at the part-of-speech-tagging and lemmatization part of the cleaning function\n",
    "## part-of-speech tagging identifies the word category (whether a word is a verb, noun, adjective, or adverb)\n",
    "print(get_wordnet_pos('running'))\n",
    "print(get_wordnet_pos('runner'))\n",
    "\n",
    "## the word categorie determines how to lemmatize the word\n",
    "lemmatizer_example = WordNetLemmatizer()\n",
    "print(lemmatizer_example.lemmatize('running',get_wordnet_pos('running')))\n",
    "print(lemmatizer_example.lemmatize('runner',get_wordnet_pos('runner')))\n",
    "print(lemmatizer_example.lemmatize('run',get_wordnet_pos('run')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f06c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split reviews in training and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34343a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply tfidf feature extraction\n",
    "\n",
    "\n",
    "## apply tfidf to training set and create vocabulary\n",
    "\n",
    "## Apply the document frequency table one the test set, to generate feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2325b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## analyze vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3669a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply and evaluate classifier on clean text data\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
