{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut9_CNN_NLP_teacher.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: Convolutional Neural Nets for Text Data\n",
    "In this tutorial, we will first explain what the layers `Conv2D` (rank-3 tensors) and `Conv1D` (rank-2 tensors) do. Then, we will use `Conv1D` to classify Tweets into positive, neutral and negative sentimentsâ€”the Tweets are from the clients of different airlines. \n",
    "\n",
    "For further examples, please visit [demos/cnn](https://github.com/Humboldt-WI/adams/tree/master/demos/cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConveNets\n",
    "Convnets are widely used in computer vision applications. The most common is the `Conv2D` which takes as input tensors of shape `(height, width, channels)` plus the batch. Let's see a simple example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3, 2), dtype=float32, numpy=\n",
       "array([[[[1., 2.],\n",
       "         [1., 2.],\n",
       "         [1., 2.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [1., 2.],\n",
       "         [1., 2.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [1., 2.],\n",
       "         [1., 2.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample input (batch, height, width, channels)\n",
    "ex_input = tf.concat([tf.ones((1,3,3,1)), 2*tf.ones((1,3,3,1))], axis=3 ) # (1,3,3,2)\n",
    "ex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_input[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 2., 2.]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_input[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 3), dtype=float32, numpy=array([[[[ 1.3313026 , -0.95462763,  0.7182502 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a convnet with 1 filter and a kernel of size 2\n",
    "cnn2D = layers.Conv2D(filters=3,kernel_size=3, input_shape=ex_input.shape[1:])\n",
    "cnn2D(ex_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand the matrix operations\n",
    "kernel = cnn2D.get_weights()#[0] # random initialization weights\n",
    "#kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.21920997, -0.09613979,  0.02231345],\n",
       "          [ 0.25881624, -0.25055763, -0.14870691]],\n",
       " \n",
       "         [[ 0.17075187,  0.12889177,  0.19216919],\n",
       "          [-0.35244083, -0.33395335, -0.17300753]],\n",
       " \n",
       "         [[-0.34580117, -0.28782514, -0.19623543],\n",
       "          [ 0.00092629,  0.05844736,  0.27339786]]],\n",
       " \n",
       " \n",
       "        [[[-0.15034717,  0.3416682 ,  0.27869117],\n",
       "          [ 0.30426514, -0.27578318, -0.00685766]],\n",
       " \n",
       "         [[ 0.36000973,  0.13831162,  0.3428462 ],\n",
       "          [-0.03165495, -0.14377864, -0.18404172]],\n",
       " \n",
       "         [[ 0.04756859, -0.07270896,  0.02796474],\n",
       "          [ 0.05926815,  0.12986422,  0.21354806]]],\n",
       " \n",
       " \n",
       "        [[[ 0.11654639,  0.20813453, -0.08173689],\n",
       "          [ 0.2831146 ,  0.15936553,  0.01463133]],\n",
       " \n",
       "         [[-0.02107   , -0.12461914,  0.34036428],\n",
       "          [-0.18139385, -0.07346916, -0.12658812]],\n",
       " \n",
       "         [[-0.19652812, -0.15600482, -0.25235897],\n",
       "          [ 0.22458053,  0.21269691,  0.15974092]]]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4641862 , -0.36797696],\n",
       "       [-0.1169914 , -0.13505894]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel[0][:,:,0,0] # weights of first channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.04840297],\n",
       "        [-0.61331284]],\n",
       "\n",
       "       [[ 0.6455597 ],\n",
       "        [-0.5659337 ]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel[:,:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.76151663"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(1*kernel[:,:,0,:])+np.sum(2*kernel[:,:,1,:]) # replicate the firts output of the convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convnets are not restricted to rank-3 tensor `(height, width, channels)`. Keras also has `Conv3D` and `Conv1D` implemented. Let's look at `Conv1D`, which requires a rank-2 tensor as input, such as sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 2), dtype=float32, numpy=\n",
       "array([[[1., 1.],\n",
       "        [2., 2.],\n",
       "        [3., 3.]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input for cnn1D (batch, seq_length, emb_dim)\n",
    "ex_input = tf.concat([tf.ones((1,1,2)), 2*tf.ones((1,1,2)), 3*tf.ones((1,1,2))], axis = 1) # (1, 3, 2)\n",
    "ex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n",
       "array([[[0.5982232 ],\n",
       "        [0.44197035]]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a convnet with 1 filter and a kernel of size 2\n",
    "cnn1D = layers.Conv1D(filters=1,kernel_size=2, input_shape=ex_input.shape[1:])\n",
    "cnn1D(ex_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = cnn1D.get_weights()[0]\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.08309507],\n",
       "        [-0.82763386]],\n",
       "\n",
       "       [[ 0.51633453],\n",
       "        [ 0.23814154]]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08309507],\n",
       "       [-0.82763386]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5982232\n",
      "0.44197035\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(1*kernel[0,:,:] + 2*kernel[1,:,:] )) # first row and second row\n",
    "print(np.sum(2*kernel[0,:,:] + 3*kernel[1,:,:] )) # secod row and third row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification\n",
    "The purpose is to put `Conv1D` into practice. We have Twitter data concerning airline clients and the labels of their tweets (positive, neutral, negative). The idea is to create a classification model for tweets. We'll only care about the positive and negative in the first part. Then, we include the neutral labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tot_tweets = pd.read_csv(\"Tweets.csv.zip\")\n",
    "tot_tweets = tot_tweets[['airline_sentiment','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive and Negative Tweets\n",
    "\n",
    "### Exercise 1: \n",
    "Remove the samples with the label `neutral`, create train and validation sets, and then transform them to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove neutral labels and transform to numpy\n",
    "tweets = tot_tweets[tot_tweets['airline_sentiment']!='neutral'].copy()\n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweets['text'], tweets['airline_sentiment'], test_size = 0.2, random_state = 5)\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "Create a function to standardize the text. In particular, replace any character that is not a-z OR A-Z with a space, convert to lowercase, and remove punctuation and double space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define standarization function \n",
    "def our_standardization(text_data):\n",
    "  remove_non = tf.strings.regex_replace(text_data, '[^a-zA-Z]', ' ') # replace non a-z OR A-Z with \" \"\n",
    "  lowercase = tf.strings.lower(remove_non) # convert to lowercase\n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  remove_punct = tf.strings.regex_replace(lowercase, pattern_remove_punctuation, '') # apply pattern\n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  remove_initial_end_spaces  =tf.strings.regex_replace(remove_double_spaces, '^\\s*|\\s*$', '')\n",
    "  return remove_initial_end_spaces\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "Create a vectorization layer and apply it to the text data. Use 10000 tokens with a maximum length for each tweet of 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "seq_length = 50\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "## Transform sequences of words to seq of integers and labels to tensor\n",
    "X_train = vectorize_layer(X_train)\n",
    "X_val = vectorize_layer(X_val)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embedding` + `Conv1D` + `MaxPooling1D` + `Flatten` + `Dense`\n",
    "### Exercise 4:\n",
    "Create a model with one `Embedding` of dimension 16, followed by a `Conv1D` with 32 filters and a kernel size of 8 and relu activation. Then, apply `MaxPooling1D` with a pool size of 2, `Flatten` the output and finally use the `Dense` layer. Can you explain the number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 16)            160000    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 43, 32)            4128      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 21, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 672)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 673       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,801\n",
      "Trainable params: 164,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filters = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: \n",
    "Train the model using a batch size of 128 for 20 epochs and an `EarlyStopping` callback with patience of 3. Restore the best weights and evaluate the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.5025 - accuracy: 0.7914 - val_loss: 0.4469 - val_accuracy: 0.7878 - 811ms/epoch - 11ms/step\n",
      "Epoch 2/20\n",
      "73/73 - 0s - loss: 0.3698 - accuracy: 0.8369 - val_loss: 0.3310 - val_accuracy: 0.8467 - 258ms/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "73/73 - 0s - loss: 0.2521 - accuracy: 0.9007 - val_loss: 0.2407 - val_accuracy: 0.9069 - 280ms/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "73/73 - 0s - loss: 0.1826 - accuracy: 0.9323 - val_loss: 0.2052 - val_accuracy: 0.9168 - 272ms/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.1461 - accuracy: 0.9471 - val_loss: 0.2048 - val_accuracy: 0.9177 - 336ms/epoch - 5ms/step\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.1232 - accuracy: 0.9546 - val_loss: 0.1945 - val_accuracy: 0.9225 - 287ms/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "73/73 - 0s - loss: 0.1051 - accuracy: 0.9623 - val_loss: 0.2064 - val_accuracy: 0.9177 - 329ms/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "73/73 - 0s - loss: 0.0917 - accuracy: 0.9683 - val_loss: 0.2069 - val_accuracy: 0.9186 - 315ms/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "73/73 - 0s - loss: 0.0797 - accuracy: 0.9737 - val_loss: 0.2201 - val_accuracy: 0.9199 - 273ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4d77c0ac0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 662us/step - loss: 0.1945 - accuracy: 0.9225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9224772453308105"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embedding` + `Conv1D` + `GlobalAveragePooling1D` + `Dense`\n",
    "### Exercise 6:\n",
    "Create a new model similar to the previous one but replace the `MaxPooling1D` and `Flatten` layers with `GlobalAveragePooling1D`. Can you explain what we are doing? Next, train the model using the previous settings. Is it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 50, 16)            160000    \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 43, 32)            4128      \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,161\n",
      "Trainable params: 164,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filters = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.5483 - accuracy: 0.7869 - val_loss: 0.5024 - val_accuracy: 0.7878\n",
      "Epoch 2/20\n",
      "73/73 - 0s - loss: 0.4441 - accuracy: 0.7971 - val_loss: 0.4142 - val_accuracy: 0.8216\n",
      "Epoch 3/20\n",
      "73/73 - 0s - loss: 0.3603 - accuracy: 0.8414 - val_loss: 0.3619 - val_accuracy: 0.8411\n",
      "Epoch 4/20\n",
      "73/73 - 1s - loss: 0.3078 - accuracy: 0.8683 - val_loss: 0.3337 - val_accuracy: 0.8553\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.2640 - accuracy: 0.8947 - val_loss: 0.2923 - val_accuracy: 0.8779\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.2256 - accuracy: 0.9143 - val_loss: 0.2636 - val_accuracy: 0.8965\n",
      "Epoch 7/20\n",
      "73/73 - 0s - loss: 0.1948 - accuracy: 0.9285 - val_loss: 0.2366 - val_accuracy: 0.9056\n",
      "Epoch 8/20\n",
      "73/73 - 0s - loss: 0.1730 - accuracy: 0.9399 - val_loss: 0.2284 - val_accuracy: 0.9142\n",
      "Epoch 9/20\n",
      "73/73 - 0s - loss: 0.1555 - accuracy: 0.9471 - val_loss: 0.2286 - val_accuracy: 0.9138\n",
      "Epoch 10/20\n",
      "73/73 - 0s - loss: 0.1417 - accuracy: 0.9513 - val_loss: 0.2128 - val_accuracy: 0.9164\n",
      "Epoch 11/20\n",
      "73/73 - 0s - loss: 0.1303 - accuracy: 0.9542 - val_loss: 0.2135 - val_accuracy: 0.9155\n",
      "Epoch 12/20\n",
      "73/73 - 0s - loss: 0.1211 - accuracy: 0.9571 - val_loss: 0.2528 - val_accuracy: 0.8991\n",
      "Epoch 13/20\n",
      "73/73 - 0s - loss: 0.1129 - accuracy: 0.9612 - val_loss: 0.2076 - val_accuracy: 0.9229\n",
      "Epoch 14/20\n",
      "73/73 - 0s - loss: 0.1065 - accuracy: 0.9637 - val_loss: 0.2100 - val_accuracy: 0.9233\n",
      "Epoch 15/20\n",
      "73/73 - 0s - loss: 0.0991 - accuracy: 0.9651 - val_loss: 0.2130 - val_accuracy: 0.9242\n",
      "Epoch 16/20\n",
      "73/73 - 0s - loss: 0.0932 - accuracy: 0.9682 - val_loss: 0.2608 - val_accuracy: 0.9034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea3aa281f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9229103326797485"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embedding` + `Conv1D` + `MaxPooling1D`+ `Conv1D` + `MaxPooling1D` + `Flatten` + `Dense`\n",
    "### Exercise 7:\n",
    "Let's try now a deeper network by adding `Conv1D` + `MaxPooling1D` to the first configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 50, 16)            160000    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 43, 32)            4128      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 21, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 18, 32)            4128      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 9, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,545\n",
      "Trainable params: 168,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filters = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(filters = num_filters, kernel_size = int(ker_size/2), activation = 'relu')(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.4905 - accuracy: 0.7900 - val_loss: 0.4461 - val_accuracy: 0.8086 - 1s/epoch - 14ms/step\n",
      "Epoch 2/20\n",
      "73/73 - 0s - loss: 0.3493 - accuracy: 0.8489 - val_loss: 0.3065 - val_accuracy: 0.8662 - 375ms/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "73/73 - 0s - loss: 0.2230 - accuracy: 0.9145 - val_loss: 0.2342 - val_accuracy: 0.9104 - 383ms/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "73/73 - 0s - loss: 0.1628 - accuracy: 0.9388 - val_loss: 0.2091 - val_accuracy: 0.9168 - 373ms/epoch - 5ms/step\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.1282 - accuracy: 0.9544 - val_loss: 0.2374 - val_accuracy: 0.9086 - 364ms/epoch - 5ms/step\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.1061 - accuracy: 0.9609 - val_loss: 0.2119 - val_accuracy: 0.9216 - 370ms/epoch - 5ms/step\n",
      "Epoch 7/20\n",
      "73/73 - 0s - loss: 0.0881 - accuracy: 0.9690 - val_loss: 0.2402 - val_accuracy: 0.9117 - 416ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4d7a821f0>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9168471097946167"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive, Negative and Neutral Tweets\n",
    "### Exercise 8:\n",
    "Now, we're going to use the three labels to create the model. But, first, encode the corresponding labels, split the data and transform it to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['airline_sentiment'] = tot_tweets['airline_sentiment'].map({'positive' : 2, 'neutral':1, 'negative': 0}).copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweets['text'], tweets['airline_sentiment'], test_size = 0.2, random_state = 5)\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9:\n",
    "Repeat the previous procedure to create the vectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "seq_length = 50\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "## Transform sequences of words to seq of integers and labels to tensor\n",
    "X_train = vectorize_layer(X_train)\n",
    "X_val = vectorize_layer(X_val)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model `Embeddings`+`Conv1D`+`MaxPooling1D`+`Flatten`+`Dense`\n",
    "### Exercise 10:\n",
    "Modify the first model, i.e. `Embeddings`+`Conv1D`+`MaxPooling1D`+`Flatten`+`Dense`, to this problem (be aware of the expected output dimension and loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_12 (Embedding)    (None, 50, 16)            160000    \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 43, 32)            4128      \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 21, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 672)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 2019      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,147\n",
      "Trainable params: 166,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "num_filtes = 32\n",
    "ker_size = 8\n",
    "\n",
    "inputs = tf.keras.Input(shape = (seq_length, ))\n",
    "emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(inputs) \n",
    "x = layers.Conv1D(filters = num_filtes, kernel_size = ker_size, activation = 'relu')(emb)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(3, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "73/73 - 1s - loss: 0.5607 - accuracy: 0.7878 - val_loss: 0.4537 - val_accuracy: 0.8146 - 699ms/epoch - 10ms/step\n",
      "Epoch 2/20\n",
      "73/73 - 0s - loss: 0.3777 - accuracy: 0.8369 - val_loss: 0.3330 - val_accuracy: 0.8419 - 286ms/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "73/73 - 0s - loss: 0.2426 - accuracy: 0.9070 - val_loss: 0.2307 - val_accuracy: 0.9117 - 281ms/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "73/73 - 0s - loss: 0.1744 - accuracy: 0.9347 - val_loss: 0.1998 - val_accuracy: 0.9238 - 276ms/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "73/73 - 0s - loss: 0.1396 - accuracy: 0.9491 - val_loss: 0.2186 - val_accuracy: 0.9168 - 275ms/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "73/73 - 0s - loss: 0.1167 - accuracy: 0.9573 - val_loss: 0.1947 - val_accuracy: 0.9212 - 268ms/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "73/73 - 0s - loss: 0.0983 - accuracy: 0.9657 - val_loss: 0.2134 - val_accuracy: 0.9117 - 281ms/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "73/73 - 0s - loss: 0.0851 - accuracy: 0.9714 - val_loss: 0.2120 - val_accuracy: 0.9160 - 269ms/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "73/73 - 0s - loss: 0.0733 - accuracy: 0.9760 - val_loss: 0.2323 - val_accuracy: 0.9199 - 282ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4d6148490>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3,restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=earlystop,\n",
    "    verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 900us/step - loss: 0.1947 - accuracy: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9211779832839966"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
