{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi0VHqNDCfYD"
   },
   "source": [
    "# Attention and transformers\n",
    "\n",
    "In this exercise you will apply a transformer model to the task of sentiment classification. We highlight two very important properties of transformer models.\n",
    "* self-attention: this mechanism enables the models to capture the context of each word within an incoming text.\n",
    "* positional encoding: Just like RNN's, transformers process sequence data, but instead of using hidden states to capture the ordering of words, transformers use positional embeddings.\n",
    "\n",
    "The first part of the exercise is meant to give an intuition of the self-attention principle. In addition to solving the exercise, you can watch the video https://www.youtube.com/watch?v=g2BRIuln4uc, which illustrates the idea of attention very clearly. Positional encodings are a rather abstract topic and will not be handled in this exercise. However, the video https://www.youtube.com/watch?v=1biZfFLPRSY offers a simple and understandable illustration of this topic.\n",
    " \n",
    "In the second part of the tutorial you need to apply a BERT (Bidirectional Encoder Representations from Transformers) model, which belongs to the family of transformer models.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mSOkyfXlHQ0_"
   },
   "outputs": [],
   "source": [
    "## required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r5IgH61j-0Oc"
   },
   "outputs": [],
   "source": [
    "## definition of the vocabulary\n",
    "voc = ['I', 'swam', 'across', 'the', 'river', 'to', 'get', 'other', 'bank', 'drove', 'road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIOUSrJz-6qZ",
    "outputId": "6c2e5d4f-02e6-42b8-e148-62cca8b85885"
   },
   "outputs": [],
   "source": [
    "## artificial two dimensional embeddings\n",
    "emb = pd.DataFrame([[1, 1], [2, 2], [1.2, 1.2], [0.9,0.9], [1.9,1.9], [0.8,0.8], [0.85,0.85], [0.95,0.95], [0,2],[2,-2],[2,-1.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq2T8NdlOlAw"
   },
   "source": [
    "### Exercise 1\n",
    "In this exercise we want to generate contextualized embeddings. Let $[v_{1},...,v_{n}]$ be a sentence were $v_{i}$ is the (non-contextualized) embedding of token $i$. A contextualized embedding $y_{i}$ of word $i$ is the weighted sum of the (non-contextualized) embeddings of the tokens in that sentence $y_{i}=\\sum_{j=1}^{n} w_{ij}v_{j}$. The $w_{ij}$ are the attention weights, which measure the importance of token $j$ for the context of token $i$.\n",
    "\n",
    "a) Your first exercise is to calculate the attention weights for each token in the sentence \"I swam across the river, to get to the other bank.\" The weights should be stored in a matrix \\begin{bmatrix}\n",
    "w_{11} & ... & w_{1n}\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "w_{n1} & ... & w_{nn}\n",
    "\\end{bmatrix} Below, we provide a function to calculate the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x_2Afu32Mm4q"
   },
   "outputs": [],
   "source": [
    "## function to calculate the attention weights\n",
    "def attention_weights(token, sent, emb,voc):\n",
    "    idx_token=voc.index(token)\n",
    "    \n",
    "    idx_sentence = []\n",
    "    for i in range(0, len(sent)):\n",
    "        idx_sentence.append(voc.index(sent[i]))\n",
    "    \n",
    "    weights = softmax([np.dot(emb.iloc[idx_token],emb.iloc[i]) for i in idx_sentence])\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bWfGJi3M4tu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exPE1enONRY7"
   },
   "source": [
    "b) Plot a heat map of the weights, using sns.heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wfD0NCxRyYM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdRJrkBDGPzE"
   },
   "source": [
    "c) Calculate the contextualized embedding of the word \"bank\" in the two sentences \"I swam across the river, to get to the other bank.\" and \"I drove across the road, to get to the other bank\". You can do this by multiplying the transposed embedding matrix $E'=\\begin{bmatrix}\n",
    "v_{1} & ... & v_{n}\\\\\n",
    "\\end{bmatrix}$ with the vector $\\begin{bmatrix}\n",
    "w_{i1} & ... & w_{in}\\\\\n",
    "\\end{bmatrix}$, of the weights of \"bank\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jj44jWGgWwaR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dmR_4EmVzVz"
   },
   "source": [
    "d) Now, you generated contextualized embeddings in a very simple way by calculating scalar products between the un-contextualized embeddings $s_{ij}=\\langle\\,v_{i},v_{j}\\rangle$, calculating weights by softmax $w_{ij}=\\frac{e^{s_{ij}}}{\\sum_{j}e^{s_{ij}}}$ and building a weighted sum of the un-contextualized embeddings $y_{i}=\\sum_{j}w_{ij}v_{j}$. Does it make sense to integrate this transformation procedure for the embeddings into a machine learning model or is there a way we could modify this procedure, such that it makes more sense?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gboIcX8X-QHg"
   },
   "source": [
    "## Exercise 2 (demonstration of BERT model use)\n",
    "In this exercise you will use a pre-trained BERT model. You will load the model and than do some fine-tuning on the model weights. We recommend to do the exercise in Google Colab because we faced some errors when loading the transformer packages on our own environment. The exercise follows the original tuturial [TF-Tutorial](https://www.tensorflow.org/text/tutorials/classify_text_with_bert). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN6Nz17SJJVi"
   },
   "source": [
    "### Install transformers\n",
    "Unlike many other libraries, Colab does not have the transformers package pre-installed. You will have to install it every time that you start Colab again. This is the package where you will find most of the critical tools for BERT including the pre-trained models and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR0LXCwYHP8l",
    "outputId": "49036900-5376-4f1b-e8dc-e83aa2e1ea40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 4.9 MB 27.4 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omuo6DMTVNPQ",
    "outputId": "65b6ea2d-6947-4307-cc75-5279fce6ec6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8 MB 40.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 92 kB 12.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 48.3 MB 145 kB/s \n",
      "\u001b[K     |████████████████████████████████| 99 kB 4.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 26.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 596 kB 50.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 45.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 352 kB 65.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 237 kB 57.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
      "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVTDahv-JdeH"
   },
   "source": [
    "### Import libraries\n",
    "Now that you have the transfomers library on hand, it will be necessary to import it and the rest of the libraries that you will need in the task. Here we will need tensorflow, pandas, OS and shutil for basic tasks and also specific parts of the transformers package for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVObl75mHYU1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub # for BERT models\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # for AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0Zn8NeZJ0DK"
   },
   "source": [
    "### Load and set up the dataset\n",
    "\n",
    "In this task, we will be using the IMDB reviews dataset. Unlike in the previous exercises, we download the data and store it in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PdO69tUHUJd",
    "outputId": "134bd996-435c-40de-e5cb-3a417d0b28d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 3s 0us/step\n",
      "84140032/84125825 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "df = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "df_dir = os.path.join(os.path.dirname(df), 'aclImdb')\n",
    "X_train_dir = os.path.join(df_dir, 'train')\n",
    "X_test_dir = os.path.join(df_dir, 'test')\n",
    "\n",
    "# we only need labeled data (data for supervised learning), so we can remove the unsupervised folder\n",
    "remove_dir = os.path.join(X_train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_uktomnWxT5"
   },
   "source": [
    "BERT is going to take up a lot of processing power. It is highly advisable to organize your data into batches so that the amount of data that you are working with is manageable. For now, we will set the size of the batches of data that we will take to 32. You can experiment with this number when working with the program for later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSaj23L2XDll"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wzMxyhMXJBq"
   },
   "source": [
    "\n",
    "Note that the function [prefetch](https://www.tensorflow.org/guide/data_performance#prefetching) is just used to prepare the data as the machine would expect to receive it. It is normally used to make sure that the next batch of data is ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcz48LGgHdAv",
    "outputId": "8528c2da-16e2-4153-8938-2c53f711efa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility in train-test split\n",
    "seed = 888\n",
    "\n",
    "# Create the pre-processing train df and create a seperate subset training only\n",
    "X_train_raw = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    X_train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "X_train = X_train_raw.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Take the validation data subset for processing\n",
    "X_val = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    X_train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "X_val = X_val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prepare the test data for processing\n",
    "X_test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    X_test_dir,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "X_test = X_test.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQrm27jOfLad"
   },
   "source": [
    "### Load a BERT model\n",
    "\n",
    "You can explore a [large list of versions of BERT here](https://huggingface.co/models). These pretrained versions of BERT differ mainly in size and/or the topics of text. Using specific versions of BERT can sometimes help with the performance of your model, though this is not always the case. It is a very good idea to test several versions of BERT for your purposes to see which one is optimal for your situation. For our purposes, we will use a small uncased BERT. Here, uncased means that BERT will ignore capitalization and small means that BERT will only take shorter inputs. \n",
    "\n",
    "As the BERT model we chose needs input of a specific format, we also need to load a customized pre-processor, which converts the text in exactly the right format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87osaWAScP8N"
   },
   "outputs": [],
   "source": [
    "## load the preprocessor\n",
    "bert_preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "\n",
    "## load the BERT model\n",
    "bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1')\n",
    "#example_bert_results = bert_model(example_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "369WRvgk6rqf"
   },
   "source": [
    "Now, we have a look how the preprocessor works. For this purpose we define an example sentence and then examine the preprocessed sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDT7m-3x66re",
    "outputId": "6f8300f4-d3ba-49ac-fed3-ffe219aac4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_word_ids: (1, 128)\n",
      "First 12 input_word_ids: [ 101 6283 2296 3371 1997 2009 1012  102    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "example_text = ['hated every minute of it.']\n",
    "example_preprocessed = bert_preprocessor(example_text)\n",
    "\n",
    "print(f'Shape of input_word_ids: {example_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'First 12 input_word_ids: {example_preprocessed[\"input_word_ids\"][0, :12]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUijb4La78pu"
   },
   "source": [
    "First, we note that input sequences for our BERT model need a sequence length of 128. This can be achieved by truncation and padding. We can see that all the words in the example sentence are converted to ID's in the vocabulary. This shows that BERT has already encountered most words that will be important for classification through its pre-training. \n",
    "\n",
    "Was the number of non-zero tokens what you expected? You may have only anticipated the following tokens: 'hated', 'every', 'minute', 'of', 'it', '.'. Why do we have an extra 2 tokens? BERT automatically adds tokens to indicate the beginning and end of a sentence as well. The rest of the sequence will be 0s as padding to keep the input length the same which is necessary for mathematical convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlq84BaGzVu_"
   },
   "source": [
    "### Build the classifier\n",
    "Now we will stack some layers to create a classifier model. We will use:\n",
    "- an input layer which receives the raw text\n",
    "- a layer to preprocess the text for the BERT encoder\n",
    "- an encoding layer which returns BERT outputs\n",
    "- a dropout layer to prevent overfitting\n",
    "- a final dense layer for the final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daXr7yxOdL2T"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  # create input layer\n",
    "  input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input text')\n",
    "  # add preprocessing layer and input text\n",
    "  preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', name='preprocessing')\n",
    "  encoder_inputs = preprocessor(input_layer)\n",
    "  # add encoding layer and feed preprocessed text into layer\n",
    "  encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  # take the pooled output and apply a dropout layer to it to prevent overfitting\n",
    "  pooled = outputs['pooled_output']\n",
    "  pooled = tf.keras.layers.Dropout(0.1)(pooled)\n",
    "  # create output layer which is the final classifier\n",
    "  pooled = tf.keras.layers.Dense(1, activation=None, name='classifier')(pooled)\n",
    "  return tf.keras.Model(input_layer, pooled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rc5aEjXmA3l0",
    "outputId": "23d72887-4092-4990-c425-4ca69a6db9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input text (InputLayer)        [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_word_ids':   0           ['input text[0][0]']             \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'encoder_outputs':  28763649    ['preprocessing[0][0]',          \n",
      "                                 [(None, 128, 512),               'preprocessing[0][1]',          \n",
      "                                 (None, 128, 512),                'preprocessing[0][2]']          \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                512),                                                             \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 512)}                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['BERT_encoder[0][5]']           \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,764,162\n",
      "Trainable params: 28,764,161\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## instantiate model\n",
    "classifier_model = build_classifier_model()\n",
    "\n",
    "## check architecture\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XyNQrxG9Pvb"
   },
   "source": [
    "### Setting up loss metric and meta parameters\n",
    "\n",
    "BERT usually gets trained for less epochs than traditional deep learning models. Depending on your task and system abilities, you can of course experiment with adding more epochs to see how it affects the model's performance. Here we set the number of epochs especially low, to get quick results during the tutorial.\n",
    "\n",
    "`steps_per_epoch` is the total number of steps (batches of observations) to yield from generator before declaring one epoch finished and starting the next epoch. We will set his equal to the cardinality (the unique items per column) as recommended by tensorflow.\n",
    "\n",
    "We will keep our learning rate at the highest level for the first 10% of training steps then it will follow a linear decay. According to the paper on BERT, you can also try learning rates of 5e-5 and 2e-5 if you'd like to experiment, but these seem to be best for fine-tuning BERT.\n",
    "\n",
    "Lastly, for an optimizer, AdamW will be used, which is Adaptive Movements with weight decay (instead of regular Adam which is based on moments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWtLED_I5zwj"
   },
   "outputs": [],
   "source": [
    "## loss\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "## meta parameters\n",
    "epochs = 1\n",
    "steps_per_epoch = tf.data.experimental.cardinality(X_train).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5 # Best options for BERT: 5e-5, 3e-5, 2e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpSBUx-Uy6Eg"
   },
   "source": [
    "Now that we have all of these set, we can compile the model with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alnfraF-52sk"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjysAvFqsFfE"
   },
   "source": [
    "### Fit and evaluate model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBOOgBA754hT",
    "outputId": "c659dcc4-9da0-4414-9f19-75457f743762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 166s 251ms/step - loss: 0.4471 - binary_accuracy: 0.7722 - val_loss: 0.3936 - val_binary_accuracy: 0.8218\n"
     ]
    }
   ],
   "source": [
    "## fit model\n",
    "history = classifier_model.fit(x=X_train,\n",
    "                               validation_data=X_val,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLpHsARDDmOG",
    "outputId": "7e47d6a7-5540-44fb-81ea-a1172c6e86db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 85s 108ms/step - loss: 0.3711 - binary_accuracy: 0.8311\n",
      "Loss: 0.37105128169059753\n",
      "Accuracy: 0.8310800194740295\n"
     ]
    }
   ],
   "source": [
    "## evaluate model\n",
    "loss_res, acc_res = classifier_model.evaluate(X_test)\n",
    "\n",
    "print(f'Loss: {loss_res}')\n",
    "print(f'Accuracy: {acc_res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8nKiP4_VMw3"
   },
   "source": [
    "## Exercise 3\n",
    "Now you have loaded a pre-trained BERT model and fine-tuned its parameters. Which parameters of the model did you modify during training compared to the pre-trained model? Load the same pre-trained model again and only train the parameters of the dense layer. Which differences do you notice? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tut10_Attention_student.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
