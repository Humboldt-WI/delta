{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut5_embeddings_teacher.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Word embeddings and Word2Vec\n",
    "The tutorial covers word embeddings in general and one of the most well-known models in this matter, the so-called Word-to-Vec (W2V). For this purpose, we reconsider the text dataset IMDB from the last tutorial. Yet, this time we preprocess the data with `Keras` using the `TextVectorization` layer that facilitates the standardization, tokenization and indexing. Next, we create a simple binary classification model using word embeddings to grasp the essence in practice. Finally, we apply the W2V model to the same data using [Gensim library](https://pypi.org/project/gensim/). \n",
    "\n",
    "Several libraries make things easier if the aim is to use W2V directly. The [Gensim library](https://pypi.org/project/gensim/) is one of them that offers a friendly interface to train embeddings, as you will see in this tutorial. \n",
    "\n",
    "However, if you would like to start from scratch and code W2V yourself using just `NumPy`, we recommend [Nathan Rooy's post](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/). Or, if you would like to do it with `TensorFlow`, there is an excellent tutorial [here](https://www.tensorflow.org/tutorials/text/word2vec) from the TensorFlow website.\n",
    "\n",
    "The outline then is the following\n",
    "1. Preparing the IMDB dataset with `Keras`\n",
    "2. Understanding embeddings with a simple binary classification model\n",
    "3. Word2Vec using `Gensim`\n",
    "\n",
    "For further examples, please visit the demo [word-2-vec.ipynb](https://github.com/Humboldt-WI/adams/blob/master/demos/nlp/word-2-vec.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the IMDB dataset\n",
    "Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Load the `IMDB-50K-Movie-Review.zip` file, and map the labels to 1 (positive) and 0 (negative). Then, have a look at the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data (be sure to provide the correct file path)\n",
    "total_imbd = pd.read_csv(\"../../../demos/nlp/IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "total_imbd['sentiment'] = total_imbd['sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "total_imbd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Split the data into training and validation sets. Use 80% of the data for training. You can use `train_test_split()` function from `sklearn`. In addition, transform the sets into `NumPy` arrays using `to_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(total_imbd['review'], total_imbd['sentiment'], test_size = 0.2, random_state = 5)\n",
    "# transform them to numpy\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have just created training and validation sets of text and labels. However, as we saw in the last tutorial, we cannot feed a neural network with this text format. We need numeric tensors. \n",
    "\n",
    "The transformation of text to numeric tensors is known as *vectorization*. This process can be split into three steps:\n",
    "1. **Standardization** of the text, such as removing punctuation, converting all the text to lowercase, etc.\n",
    "2. **Tokeinzation** of the standardized text, where we separate the text into units or *tokens*, usually words or n-grams.\n",
    "3. **Indexing** of the tokens into a numerical vector.\n",
    "\n",
    "These 3 steps are implemented in the Keras `TextVectorization` layer.\n",
    "\n",
    "```python\n",
    "TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "```\n",
    "Where `our_standardization` is our customized function to standardize the text, for example, we saw that some examples in the dataset have HTML tags `<br />`, and we'd like to delete them. \n",
    "\n",
    "### Exercise 3\n",
    "So, let's first build our own standardization function called `our_standardization`. The function should convert uppercase to lowercase (`tf.strings.lower`), remove HTML tags (`tf.strings.regex_replace`), deletes the punctuation (`re.escape(string.punctuation)`) and double spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_standardization(text_data):\n",
    "  lowercase = tf.strings.lower(text_data) # convert to lowercase\n",
    "  remove_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # remove HTML tags\n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  remove_punct = tf.strings.regex_replace(remove_html, pattern_remove_punctuation, '') # apply pattern\n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  return remove_double_spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Apply the `our_standardization` function to the following text and see how it works\n",
    "\n",
    "`\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 12:05:33.579317: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'bruce dern also is in the mix and dern never fails to fascinate in about any film the movie could be considered kind of downer to the average viewer but i found it fascinatingand i dont like depressing movies normally what i found was a kind of quirky crime film'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the our_standardization function\n",
    "our_standardization(\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Great! So let's now vectorize our data (use `TextVectorization`) with a vocabulary of the first 10,000 most frequent words and a maximum sequence of the text of 100 characters. Called this layer `vectorize_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the vocabulary and the max number of words in a sequence\n",
    "vocab_size = 10000\n",
    "seq_length = 100\n",
    "\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Index the vocabulary. To do it, you need to call the `adapt()` method from the `vectorize_layer` and apply it to `X_train`. Then, retrieve the computed vocabulary using `get_vocabulary()` and save it into `vocab`. Finally, print the first 10 words of the vocabulary (`print(vocab[:10])`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "# To create the vocabulary, we need to call adapt. The input is only the text\n",
    "vectorize_layer.adapt(X_train)\n",
    "# Check the first 10 words of the vocabulary. It is sorted by frequency \n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Apply the `vectorization_layer` to the same previous example, i.e. To\n",
    "\n",
    "`\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
       "array([[1389, 8562,   82,    7,    8,    2, 1579,    4, 8562,  109,  976,\n",
       "           6,    1,    8,   42,   99,   19,    2,   17,   97,   26, 1206,\n",
       "         236,    5,    1,    6,    2,  871,  527,   18,   10,  245,    9,\n",
       "           1,   10,   89,   38, 2234,   92, 1805,   48,   10,  245,   13,\n",
       "           3,  236,    5, 2653,  832,   19,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the vectorization layer\n",
    "vectorize_layer([\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding word embeddings\n",
    "We have not introduced any embedding layer until now; instead, we created a vectorization layer that can transform text inputs into numeric tensors. So, for example, the text \"tomorrow is Saturday\" will be transformed into something like `[23, 45, 5, 0, 0, 0]` (if the length of the sequence is 6). An embedding layer transforms each word, which can be thought of as a one-hot vector, into another more dense vector. \n",
    "\n",
    "Let's see an example of the `Embedding` layer in Keras, where we hypothetically have only 100 words in the vocabulary, and we want to transform this space into a 5-dimensional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1: (6, 5) \n",
      "result2: (2, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create the embedding layer of shape (100,5)\n",
    "embedding_layer = layers.Embedding(100, 5)\n",
    "# Feed a sequence of word indices\n",
    "result1 = embedding_layer(tf.constant([23, 45, 5, 0, 0, 0]))\n",
    "# We can also feed batches    \n",
    "result2 = embedding_layer(tf.constant([[23, 45, 5, 0, 0, 0], [3, 4, 55, 4, 0, 0]]))\n",
    "print(\"result1:\",result1.shape,\"\\nresult2:\",result2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word index has been transformed into a 5-dimensional vector (in this case, random values). For example, the values of `results1` are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[-0.01591078,  0.03600693, -0.04597142, -0.01586562,  0.02273249],\n",
       "       [ 0.03328559, -0.01752152, -0.00554677, -0.00369354,  0.02254558],\n",
       "       [ 0.0063172 ,  0.02094451,  0.01136215,  0.02023139,  0.0041159 ],\n",
       "       [-0.00736328,  0.01393194,  0.0357581 , -0.02324574, -0.0478739 ],\n",
       "       [-0.00736328,  0.01393194,  0.0357581 , -0.02324574, -0.0478739 ],\n",
       "       [-0.00736328,  0.01393194,  0.0357581 , -0.02324574, -0.0478739 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "Build a simple model to infer the sentiment. To do it, use a `Sequential` model where the first layer transforms the text into tensors (`vectorize_layer`), the second layer embeds the vocabulary into a 16-dimension (`layers.Embedding`), the third layer uses `layers.GlobalAveragePooling1D()` to reduce the complete text to a single average vector in the embedding space, and finally, use a `Dense` layer with a sigmoid activation to infer the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model to use word embeddings\n",
    "embedding_dim = 16\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"), \n",
    "  layers.GlobalAveragePooling1D(), # each sample is reduced to the average of the word embeddings\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 \n",
    "Compile the model with the `rmsprop` optimizer, the adequate loss function and monitor the `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compile it\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', # positive or negative\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "Fit the model using 10 `epochs`. Remember to specify the validation dataset in `validation_data`. How accurate is the model in the validation set? How many parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.6582 - accuracy: 0.6827 - val_loss: 0.5239 - val_accuracy: 0.7822\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.4844 - accuracy: 0.7939 - val_loss: 0.4186 - val_accuracy: 0.8149\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3842 - accuracy: 0.8340 - val_loss: 0.3835 - val_accuracy: 0.8269\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3499 - accuracy: 0.8474 - val_loss: 0.3682 - val_accuracy: 0.8351\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3304 - accuracy: 0.8570 - val_loss: 0.3617 - val_accuracy: 0.8398\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3224 - accuracy: 0.8638 - val_loss: 0.3587 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3080 - accuracy: 0.8698 - val_loss: 0.3582 - val_accuracy: 0.8398\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3037 - accuracy: 0.8717 - val_loss: 0.3582 - val_accuracy: 0.8396\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.3025 - accuracy: 0.8714 - val_loss: 0.3584 - val_accuracy: 0.8408\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.2959 - accuracy: 0.8774 - val_loss: 0.3584 - val_accuracy: 0.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa1cbac0640>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~ 2 minutes\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data = (X_val, y_val),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_2 (TextVe (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,017\n",
      "Trainable params: 160,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the number of trainable parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11\n",
    "We would like to visualize the trained embbedings. For this purpose, first, get the weights of the embedding layer with `get_weights()` module. Save them in `embeddings`. Then run the following code to save the embeddings and labels in the correct format to visualize them with [https://projector.tensorflow.org/](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the embeddings! is a matrix of shape (vocab_size, embedding_dimension).\n",
    "embeddings = model.get_layer('embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the embeddings to visualize them with https://projector.tensorflow.org/\n",
    "embeddings_doc = io.open('simple_mod_tensor.tsv', 'w', encoding='utf-8')\n",
    "words_doc = io.open('simple_mod_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "  if i == 0:\n",
    "    continue  # skip the padding\n",
    "  embedding = embeddings[i]\n",
    "  embeddings_doc.write('\\t'.join([str(x) for x in embedding]) + \"\\n\")\n",
    "  words_doc.write(word + \"\\n\")\n",
    "embeddings_doc.close()\n",
    "words_doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go and check how these embeddings look in 2D or 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with Gensim library\n",
    "Let's now apply W2V to the same text data. Remember that W2V proposes two models for learning word vectors, continuous-bag-of-words (CBOW) and Skip-Gram (SG). In a nutshell, CBOW predicts a central <font color='yellow'>target</font> word from surrounding <font color='green'>context</font> words, while SG takes the opposite approach. Given a <font color='yellow'>target</font> word, predict <font color='green'>context</font> words. For example, using a window size of 2 to the following phrase\n",
    "\n",
    "> I finally <font color='green'>found a</font><font color='yellow'> machine</font><font color='green'> at the </font>  gym that I like: the vending machine!\n",
    "\n",
    "So in CBOW, the problem is \n",
    "\n",
    "[I finally <font color='green'>found a</font><font color='yellow'> ?</font><font color='green'> at the </font> gym]\n",
    "\n",
    "And in SG,\n",
    "\n",
    "[I finally <font color='green'>? ?</font><font color='yellow'> machine</font><font color='green'> ? ? </font> gym]\n",
    "\n",
    "In this tutorial, we apply SG (argument `sg=1` in `gensim`). But you are welcome to compare results for CBOW (`sg=0`).\n",
    "\n",
    "### Exercise 12\n",
    "Create `X_train_vec` by applying the vectorization layer you have already created to `X_train`. In this way, we will be using the same vectorization procedure as before (same vocabulary, length of the sequence, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vectorize_layer to X_train\n",
    "X_train_vec = vectorize_layer(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "Gensim accepts words, so convert the `X_train_vec` into a list of words. Called this object `X_train_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 6 min\n",
    "X_train_words = [[vocab[w] for w in rev] for rev in X_train_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14\n",
    "Train a W2V model using `Word2Vec` function and `X_train_words` as input (call it `w2v_model`). Use `min_count` of 1, a `window` of 5, 50 `epochs`, a `vector_size` of 100 for the embeddings and SG (`sg=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10000, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model ~ 5 min\n",
    "w2v_model = Word2Vec(X_train_words, #X_train_vec.numpy(), \n",
    "                 min_count=1,  #min_count means the frequency benchmark\n",
    "                 window=5,     #the size of context\n",
    "                 epochs=50,  \n",
    "                 vector_size=100, #size of embedding\n",
    "                 workers=4,#for parallel computing\n",
    "                 sg  = 1)    \n",
    "# summarize the loaded model\n",
    "print(w2v_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15\n",
    "Check how similar are great to good, great to horrible and so on. Use `w2v_model.wv.similarity()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69516975\n",
      "0.3987277\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.similarity('great', 'good'))\n",
    "print(w2v_model.wv.similarity('great', 'horrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16 \n",
    "Get the `topn` 5 most similar words to great. Use `w2v_model.wv.most_similar` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wonderful', 0.8117287158966064),\n",
       " ('fantastic', 0.7801386117935181),\n",
       " ('fine', 0.7010308504104614),\n",
       " ('good', 0.6951696872711182),\n",
       " ('terrific', 0.6847767233848572)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('great', topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following code to save the embeddings and labels so we can visualize them with https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this matrix to visualize it in https://projector.tensorflow.org/\n",
    "w2v_model.wv.save_word2vec_format(\"word2vec.model\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec.model\", binary=False)\n",
    "embeddings_doc = io.open('w2v_mod_tensor.tsv', 'w', encoding='utf-8')\n",
    "words_doc = io.open('w2v_mod_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word in model.index_to_key:\n",
    "    vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "    word=str(\"PAD\") if word==\"\" else word\n",
    "    words_doc.write(word + \"\\n\")\n",
    "    embeddings_doc.write(vector_row + \"\\n\")\n",
    "\n",
    "embeddings_doc.close()\n",
    "words_doc.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('adams')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
