{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Training and Tuning of Deep Learning Models in Python\n",
    "\n",
    "The lecture has introduced you to foundations of neural networks. Therefore, in today's tutorial notebook we will revisit the forward pass performed in shallow and deep neural networks on tabular data. This will help us understand the functionality behind **sklearn's MLPRegressor**, as well as which hyperparameters of neural networks can be tuned to improve their performance. \n",
    "\n",
    "Here is the outline of the notebook:\n",
    "*   Forward pass of shallow and deep neural networks in numpy (Demo). \n",
    "*   Implementation of two MLPRegressor models with varying depth using sklearn (Excercise 1).\n",
    "*   Tuning of MLPRegressor using Optuna (Excercise 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Forward Pass of shallow and deep Neural Networks with Numpy** (DEMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Forward Pass in Neural Networks with a single hidden Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's revisit the forward pass in a shallow neural network, using the below illustration: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shallow Neural Network](https://github.com/Humboldt-WI/demopy/raw/main/Shallow_Neural_Network_Forward_Pass.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a neural network with **<em>m</em>** input variables, which are fed to a single hidden layer with **<em>w</em>** = 2 units. The hidden layer makes use of the activation function **<em>g</em>** to transform the latent representation of the inputs nonlinearly. The output of the nonlinear transformation is then fed to the final layer, i.e., the prediction layer, which produces the estimates of the target variable **<em>y</em>**.<br>\n",
    "\n",
    "Now, imagine you have a batch of 20 samples with 5 predictor features and a single continuous target variable. You are tasked to implement the forward pass of the above-described neural network with numpy. First, we will simulate our batch of data by drawing samples from a standard normal distribution using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch of simulated data:  (20, 5)\n"
     ]
    }
   ],
   "source": [
    "#Simulation of a batch of 20 samples with 5 features:\n",
    "w=2\n",
    "m=5\n",
    "num_samples=20\n",
    "data_batch=np.random.normal(size=m*num_samples,loc=0.0,scale=1.0).reshape(num_samples,m)\n",
    "#We create an artificial target variable using the exponential function in numpy:\n",
    "target=np.exp(np.dot(data_batch,np.random.normal(size=m).reshape(-1,1)))\n",
    "print('Shape of batch of simulated data: ',data_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize the trainable parameters of the hidden layer and the output layer of the shallow network. The shape of the weight matrices in the hidden layer  and the output layer are **<em>(m,w)</em>** and **<em>(w,1)</em>**. The shape of the bias weights is determined by the  number of units in each layer. Since one of the main advantages of using neural networks is their ability to approximate nonlinear relationships, we will define two nonlinear activation functions, i.e., ReLU and Tanh. We will use both to nonlinearly transform the latent feature space produced by the matrix multiplication of the network weights with the synthetic batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the trainable weight matrices of the shallow neural network:\n",
      "- W of hidden layer: (5, 2)\n",
      "- Bias of hidden layer: (2,) \n",
      "\n",
      "- W of output layer: (2, 1)\n",
      "- Bias of output layer: (1,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_hidden_layer=np.random.normal(size=m*w).reshape(m,w)\n",
    "bias_hidden_layer=np.random.normal(size=w)\n",
    "\n",
    "w_output_layer=np.random.normal(size=w).reshape(-1,1)\n",
    "bias_output_layer=np.random.normal(size=1)\n",
    "\n",
    "print('Shape of the trainable weight matrices of the shallow neural network:')\n",
    "print('- W of hidden layer:',w_hidden_layer.shape)\n",
    "print('- Bias of hidden layer:',bias_hidden_layer.shape,'\\n')\n",
    "print('- W of output layer:',w_output_layer.shape)\n",
    "print('- Bias of output layer:',bias_output_layer.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the linear latent representation of the input variables:  (20, 2)\n"
     ]
    }
   ],
   "source": [
    "#Linear latent feature space of inputs: generated with matrix multiplication,\n",
    "#we implement the latter with the function numpy.dot:\n",
    "linear_latent_representation=np.dot(data_batch,w_hidden_layer) + bias_hidden_layer\n",
    "print('Shape of the linear latent representation of the input variables: ',linear_latent_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication of the weights with the input variables produces a linear latent representation consisting of 2 units for each of the 20 synthetic data samples. The bias weights array has only 2 trainable weights. At first sight, the addition of one-dimensional array to a two-dimensional matrix with the shape (20,2) seems conceptually impossible in the context of linear algebra. However, due to broadcasting, numpy efficiently adds the bias weights to each row of the latent representation. For more details on broadcasting, we refer the reader to: https://numpy.org/doc/stable/user/basics.broadcasting.html. <br>\n",
    "\n",
    "Next, we create a well-documented function implementing ReLU and Tanh: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_func(x:np.ndarray,\n",
    "                   activation_name:str)->np.ndarray:\n",
    "    \"\"\"Computes ReLU or Tanh transformations of the inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "        The linearly transformed inputs.\n",
    "    activation_name: str\n",
    "        The name of the activation function, i.e., ReLU or Tanh.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    nonlinear_x: np.ndarray\n",
    "        The nonlinearly transformed inputs.\n",
    "    \n",
    "    Raises:\n",
    "    ----------\n",
    "    ValueError: If specified activation_name is not in the list [ReLU, Tanh].\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_name not in ['ReLU','Tanh']:\n",
    "        raise ValueError(f'The currently specified activation function {activation_name} is not in the list of supported nonlinearities.')\n",
    "        \n",
    "    if activation_name=='Tanh':\n",
    "        nonlinear_x=np.tanh(x)\n",
    "    else:\n",
    "        nonlinear_x=x*(x>0.0)\n",
    "        \n",
    "    return nonlinear_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above function, we specify the type of the input parameters and the type of the output values of our function. Also, we provide a brief summary of what the function does, what the input parameters and the return value represent. Proper documentation of your functions improves the readability of your code a lot especially for someone, who is examining your code for the first time. For more details on the annotation of python functions, we refer the reader to: https://peps.python.org/pep-3107/#fundamentals-of-function-annotations. <br>\n",
    "\n",
    "Next, we will pass the linearly weighted inputs through our nonlinear activations, and we will plot the distribution of the resulting values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu_latent=nonlinear_func(x=linear_latent_representation,activation_name='ReLU')\n",
    "tanh_latent=nonlinear_func(x=linear_latent_representation,activation_name='Tanh')\n",
    "\n",
    "fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(10,3))\n",
    "ax[0].hist(relu_latent.flatten())\n",
    "ax[0].set_title('ReLU latent Feature Space')\n",
    "\n",
    "ax[1].hist(tanh_latent.flatten())\n",
    "ax[1].set_title('Tanh latent Feature Space')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram plots highlight the difference in the distribution of the latent features. While ReLU produces a right-skewed representation of the inputs, Tanh generates a bimodal latent feature space. Generally, the ReLU activation is prefered over Tanh in neural networks, especially in deep neural models. However, Tanh could also be applied in rather shallow neural models. The main limitation of Tanh, and generally speaking of activations with a lower and an upper bound, is that output values very close to the saturation regions, i.e., -1 and 1 in case of Tanh, receive almost no gradients in the backward pass. Therefore, bounded activation functions suffer from the vanishing gradients problem.<br>\n",
    "\n",
    "Next, we produce the predictions of our synthetic target:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape:  (20, 1)\n"
     ]
    }
   ],
   "source": [
    "relu_predictions=np.dot(relu_latent,w_output_layer)+bias_output_layer\n",
    "tanh_predictions=np.dot(tanh_latent,w_output_layer)+bias_output_layer\n",
    "print('Predictions shape: ',relu_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of the estimations, we define a function computing the mean-squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(true_target:np.ndarray,\n",
    "        predictions:np.ndarray)->np.float64:\n",
    "    '''\n",
    "    Computes the mean-squared error (MSE) between the true values of the target variable and the predictions of an ML model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_target: np.ndarray\n",
    "        The true values.\n",
    "    predictions: np.ndarray\n",
    "        The estimation of the target values.\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    mse_score: np.float\n",
    "        The error between the true values and the predictions measured in terms of MSE.\n",
    "    '''\n",
    "    mse_score=np.mean((true_target-predictions)**2)\n",
    "    return mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE from ReLU-Network:  36.464\n",
      "MSE from Tanh-Network:  34.745\n"
     ]
    }
   ],
   "source": [
    "print('MSE from ReLU-Network: ',round(mse(true_target=target,predictions=relu_predictions),3))\n",
    "print('MSE from Tanh-Network: ',round(mse(true_target=target,predictions=tanh_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have performed the forward pass of a neural network with a single hidden layer. Without any training of the two versions of the neural network the results in terms of MSE seem pretty close to each other, as both versions use the same randomly initialized weights in the hidden layer. We will use our mse function in Section 2 and Section 3, where we will compare the performance of two trained neural networks and we will implement neural network tuning, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Forward Pass in deep Neural Networks with several hidden Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to more coding, let's recap the forward pass in deep neural networks: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Neural Network](https://github.com/Humboldt-WI/demopy/raw/main/Deep_Neural_Networks_Forward_Pass.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization shows that in a deep neural network the layers are structured in a chain, where the output of each layer is fed as an input to every following layer. By \"fed\", we mean that we compute the matrix multiplication between the latent representation of the previous layer and the trainable weights of the current layer, add the biases and apply a nonlinear activation function. Therefore, the output shape of each hidden layer determines the shape of the weight matrices of every following hidden layer.<br>\n",
    "\n",
    "Below, we create a function, which computes the forward pass of an arbitrarily ReLU-based deep neural network implemented in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_net(hidden_units:list,\n",
    "                    inputs:np.ndarray,\n",
    "                    print_layer_shape:bool)->np.ndarray:\n",
    "    '''\n",
    "    Computes the forward pass with an arbitrarily deep feedforward neural network.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hidden_units: list\n",
    "        A list of hidden units, which also determines the number of hidden layers in the network.\n",
    "    inputs: np.ndarray\n",
    "        A two-dimensional numpy matrix containing the data samples in the rows, and the predictor variables in the columns.\n",
    "    print_layer_shape: bool\n",
    "        Determines whether to print the shape of the latent representation produced by each hidden layer and the final prediction layer.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    predictions: np.ndarray\n",
    "        The estimation of the target.\n",
    "    '''\n",
    "    \n",
    "    current_linear_representation=inputs.copy()\n",
    "    if print_layer_shape==True:\n",
    "        print('Inputs shape: ',inputs.shape,'\\n')\n",
    "    \n",
    "\n",
    "    for nr_units_idx in range(0,len(hidden_units)):\n",
    "        if nr_units_idx==0:\n",
    "            current_weights=np.random.normal(size=inputs.shape[1]*hidden_units[nr_units_idx]).reshape(inputs.shape[1],hidden_units[nr_units_idx])\n",
    "        else:\n",
    "            current_weights=np.random.normal(size=previous_matrix_shape[1]*hidden_units[nr_units_idx]).reshape(previous_matrix_shape[1],hidden_units[nr_units_idx])\n",
    "        current_bias_weights=np.random.normal(size=hidden_units[nr_units_idx])\n",
    "\n",
    "        #At each hidden layer we overwrite the linear representation passed to the nonlinearity: \n",
    "        current_linear_representation=np.dot(current_linear_representation,current_weights)+current_bias_weights\n",
    "\n",
    "        #ReLU-based transformation:\n",
    "        current_nonlinear_representation=nonlinear_func(x=current_linear_representation,\n",
    "                                                        activation_name='ReLU')\n",
    "        previous_matrix_shape=current_weights.shape\n",
    "\n",
    "        if print_layer_shape==True:\n",
    "            print('Output shape of ',(nr_units_idx+1),'.hidden layer: ',current_nonlinear_representation.shape,'\\n')\n",
    "    \n",
    "    #Compute the predictions:\n",
    "    output_layer_weights=np.random.normal(size=current_nonlinear_representation.shape[1]).reshape(-1,1)\n",
    "    bias_output_layer=np.random.normal(size=1).reshape(-1,1)\n",
    "    \n",
    "    predictions=np.dot(current_nonlinear_representation,output_layer_weights)+bias_output_layer\n",
    "    if print_layer_shape==True:\n",
    "        print('Output shape of prediction layer: ',predictions.shape,'\\n')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:  (20, 5) \n",
      "\n",
      "Output shape of  1 .hidden layer:  (20, 100) \n",
      "\n",
      "Output shape of  2 .hidden layer:  (20, 50) \n",
      "\n",
      "Output shape of  3 .hidden layer:  (20, 10) \n",
      "\n",
      "Output shape of prediction layer:  (20, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_deep_net=deep_neural_net(hidden_units=[100,50,10],\n",
    "                                     inputs=data_batch,\n",
    "                                     print_layer_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Implementation of deep feed-forward regression neural networks using sklearn.** <br>(Excercise 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, first, we will import a house pricing dataset from tensorflow, i.e., boston housing dataset. Afterward, we will examine some basic statistical characteristics of the features. Once the data is rescaled, we will train an MLPRegressor from sklearn to forecast the median price values of boston houses (our target variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (404, 13) \n",
      "Shape of test data: (102, 13)\n"
     ]
    }
   ],
   "source": [
    "# this will import and split the data 80-20% by default:\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "print('Shape of train data:',x_train.shape,\n",
    "'\\nShape of test data:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the features are the following (more details about this dataset can be found in [http://lib.stat.cmu.edu/datasets/boston](http://lib.stat.cmu.edu/datasets/boston))<br>\n",
    "| # | Variable | Description |\n",
    "|---|---|---|\n",
    "| 1 | CRIM | per capita crime rate by town |\n",
    "| 2 | ZN | proportion of residential land zoned for lots over 25,000 sq.ft. |\n",
    "| 3 | INDUS | proportion of non-retail business acres per town |\n",
    "| 4 | CHAS | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |\n",
    "| 5 | NOX | nitric oxides concentration (parts per 10 million) |\n",
    "| 6 | RM | average number of rooms per dwelling |\n",
    "| 7 | AGE | proportion of owner-occupied units built prior to 1940 |\n",
    "| 8 | DIS | weighted distances to five Boston employment centres |\n",
    "| 9 | RAD | index of accessibility to radial highways |\n",
    "| 10 | TAX | full-value property-tax rate per $10,000 |\n",
    "| 11 | PTRATIO | pupil-teacher ratio by town |\n",
    "| 12 | B | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town |\n",
    "| 13 | LSTAT | % lower status of the population |\n",
    "\n",
    "So, we have 404 samples to train and 102 to test, each with 13 numerical features. The target is the median values of homes. Let's have a quick look at the predictor features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.745111</td>\n",
       "      <td>11.480198</td>\n",
       "      <td>11.104431</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.557356</td>\n",
       "      <td>6.267082</td>\n",
       "      <td>69.010644</td>\n",
       "      <td>3.740271</td>\n",
       "      <td>9.440594</td>\n",
       "      <td>405.898515</td>\n",
       "      <td>18.475990</td>\n",
       "      <td>354.783168</td>\n",
       "      <td>12.740817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.240734</td>\n",
       "      <td>23.767711</td>\n",
       "      <td>6.811308</td>\n",
       "      <td>0.241238</td>\n",
       "      <td>0.117293</td>\n",
       "      <td>0.709788</td>\n",
       "      <td>27.940665</td>\n",
       "      <td>2.030215</td>\n",
       "      <td>8.698360</td>\n",
       "      <td>166.374543</td>\n",
       "      <td>2.200382</td>\n",
       "      <td>94.111148</td>\n",
       "      <td>7.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.874750</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.077100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.225000</td>\n",
       "      <td>374.672500</td>\n",
       "      <td>6.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.198500</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>3.142300</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>11.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.674808</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>6.609000</td>\n",
       "      <td>94.100000</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.157500</td>\n",
       "      <td>17.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.725000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>10.710300</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.745111   11.480198   11.104431    0.061881    0.557356    6.267082   \n",
       "std      9.240734   23.767711    6.811308    0.241238    0.117293    0.709788   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081437    0.000000    5.130000    0.000000    0.453000    5.874750   \n",
       "50%      0.268880    0.000000    9.690000    0.000000    0.538000    6.198500   \n",
       "75%      3.674808   12.500000   18.100000    0.000000    0.631000    6.609000   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.725000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.010644    3.740271    9.440594  405.898515   18.475990  354.783168   \n",
       "std     27.940665    2.030215    8.698360  166.374543    2.200382   94.111148   \n",
       "min      2.900000    1.129600    1.000000  188.000000   12.600000    0.320000   \n",
       "25%     45.475000    2.077100    4.000000  279.000000   17.225000  374.672500   \n",
       "50%     78.500000    3.142300    5.000000  330.000000   19.100000  391.250000   \n",
       "75%     94.100000    5.118000   24.000000  666.000000   20.200000  396.157500   \n",
       "max    100.000000   10.710300   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  404.000000  \n",
       "mean    12.740817  \n",
       "std      7.254545  \n",
       "min      1.730000  \n",
       "25%      6.890000  \n",
       "50%     11.395000  \n",
       "75%     17.092500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desriptive statistics show that the is a big variation in the mean and the standard deviation of the predictor variables. This indicates that the input features have different scales. For this reason, we will standardize our test dataset based on the statistics computed on the rescaled train dataset. With data rescaling, we ensure that no predictor dominates the input feature space due to varying variable scale. This, in turn, improves the stability of the training process, and facilitates faster convergence. The StandardScaler can be imported from sklearn.preprocessing: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. Take a look at the code example provided under the link, and standardize the boston housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import ...\n",
    "#Data rescaling:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the distribution of the target variable on the train set, and examine whether it is also necessary to standardize the target in addition to the predictor variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the distribution of target train values with a histogram from matplotlib.pyplot (imported as plt):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are completely done with the data preprocessing, we move on to training MLPRegressor() from sklearn.neural_network: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html. After examining the possible hyperparameters, which can be specified in MLPRegressor(), we will train two neural models. The latter should use the Adam optimizer with the learning rate=0.001. Additionally, the first neural network should have a single Tanh-based hidden layer with 10 units, while the second network should apply ReLU in two hidden layers with the number of units [10,5]. Once the training process is complete, we will compare the performance of the two models on the test set using MSE. We defined the latter in a function in the first part of the notebook (the demo showing the forward passes). If you re-run the models several times, do you observe differences in the results? If yes, then why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import ...\n",
    "\n",
    "#Your implementation of the MLPRegressors: \n",
    "tanh_MLP=...\n",
    "relu_MLP=...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Hyperparameter Tuning using Optuna.** <br>(Excercise 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna represents an automatic hyperparameter optimization software framework, which supports multiple samplers, including the Tree-Parzen Estimator (TPE). This name implies a tree-structured search space, i.e., a search space that includes some conditional parameters, and use Parzen estimators such as kernel density estimators (KDEs). In a nutshell, TPE models the probability distribution of good and bad hyperparameter configurations, and then uses that to guide the search, i.e., to achieve a balance between exploration (trying new areas) and exploitation (focusing on promising regions).<br>\n",
    "\n",
    "Before we move on with the encoding of the search space using optuna, take a look at some code examples under the link: https://optuna.org/#code_examples. The hyperparameter tuning with optuna consists mainly of 5 aspects:\n",
    "- the definition of an objective function, which encodes the potentially conditional search space.\n",
    "- the creation of a study with optuna using a specific sampler, e.g., TPE. The study object should also  <br> \n",
    "  receive the direction for the optimization of the chosen metric, i.e., maximize accuracy or minimize an error metric.\n",
    "- the optimization of the hyperparameters for a pre-defined number of trials based on a score computed on the validation set. \n",
    "- extraction of the best hyperparameter configuration, and application of the best performing model on the test set.\n",
    "\n",
    "Next, we will reserve the last 104 samples from our train dataset for validation purposes, and we will define the following search space in Optuna's objective function:\n",
    "- for a maximum of 3 hidden layers sample values in [10,200] with stepsize=10. For deeper hidden layers allow optuna to sample also 0 units, indicating the network is not grown deeper.\n",
    "- Optimizer: ['adam','sgd']\n",
    "- Learning_Rate: [0.0001,0.001] with stepsize=0.0001\n",
    "- Activation=['identity', 'logistic', 'tanh', 'relu']\n",
    "\n",
    "Additionally, the alpha parameter, i.e., the L2 regularization coefficient, is set to the rather high value of 0.01 during the tuning process to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#We reserve 104 samples from our training dataset for validation purposes. We should rescale the new\n",
    "#train and validation subsets: \n",
    "x_train_optuna,x_val_optuna=...\n",
    "y_train_optuna,y_val_optuna=...\n",
    "\n",
    "#We rescale the new train and validation subsets:\n",
    "\n",
    "x_train_optuna_rescaled=...\n",
    "x_val_optuna_rescaled=...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main structure of the objective function is already defined. Therefore, you would have to fill in the missing code in order to create the search space. If the code runs error free, you should be able to execute the optimization (two cells below the current cell, where we call study.optimize). The comments in the objective function highlight the functionality that has to be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the objective as python function: \n",
    "\n",
    "def objective(trial: optuna.Trial) -> np.float64:\n",
    "    '''\n",
    "    Perform hyperparameter tuning using optuna.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial:optuna.Trial\n",
    "        An optuna run with a specific hyperparameter configuration sampled in this function.\n",
    "\n",
    "    Returns: \n",
    "    ---------\n",
    "    mse_val: np.float64\n",
    "        The mse error score achieved on the validation set.\n",
    "    '''\n",
    "\n",
    "    #Sample number of neurons for a pre-defined maximum number of hidden layers: \n",
    "    # here you need to consider conditional hyperparameters:\n",
    "    max_hidden_layers=3\n",
    "    hidden_units_list=[]\n",
    "    keep_sampling_hidden_units=True\n",
    "    nr_hidden_layer=0\n",
    "\n",
    "    #Keep on sampling the number of hidden units until you have reached the max number of layers\n",
    "    #or until optuna has sampled 0 units indicating the network should not be grown deeper:\n",
    "    while keep_sampling_hidden_units:\n",
    "        if nr_hidden_layer==0:\n",
    "            current_hidden_units=trial.suggest_int(\"Units_hidden_layer_\"+str(nr_hidden_layer), low=10, high=200,step=10)\n",
    "        else:\n",
    "            current_hidden_units=...\n",
    "        \n",
    "        #Terminate the training if max number of layers reached or currently sampled units are 0:\n",
    "        ...\n",
    "        \n",
    "        #Save the sampled number of units in the list hidden_units_list unless optuna has sampled 0 units:\n",
    "        ...\n",
    "        \n",
    "            \n",
    "        nr_hidden_layer=nr_hidden_layer+1\n",
    "    \n",
    "    #Non-conditional hyperparameters:\n",
    "    current_optimizer=...\n",
    "    current_learning_rate=...\n",
    "    current_activation=...\n",
    "    \n",
    "    #Implementation of an MLPRegressor using the hyperparameters sampled in the current trial.\n",
    "    #Don't forget to also set the L2 parameter to a rather high value, e.g., 0.01, to avoid overfitting:\n",
    "    current_MLP_regressor=...\n",
    "    \n",
    "    #Fit on the training set and evaluate the performance on validation set:\n",
    "    current_MLP_regressor.fit(x_train_optuna_rescaled,y_train_optuna)\n",
    "    val_predictions=current_MLP_regressor.predict(x_val_optuna_rescaled)\n",
    "\n",
    "    mse_val=...\n",
    "    return mse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization using Optuna for 50 trials:\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(),direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction of best hyperparameters and application of the model trained with the best configuration on the test set:\n",
    "best_config=study.best_trial.params\n",
    "print('Best configuration:')\n",
    "for key in best_config.keys():\n",
    "    print(key,': ',(best_config[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of the MLP-regressor with the best hyperparameter configuration:\n",
    "best_mlp_regressor=...\n",
    "\n",
    "best_mlp_regressor.fit(x_train_rescaled,y_train)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison of results with tuned vs not tuned ReLU-neural network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2c68f0abcf3705a60d2a1ddd11382db992c60d9e301bdf771b0d9399403a061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
