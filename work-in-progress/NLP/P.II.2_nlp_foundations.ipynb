{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyuoS1V_A74Q"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/demos/nlp/nlp_foundations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skvR6YA3A74T"
   },
   "source": [
    "# Fundamentals of natural language processing (NLP)\n",
    "Pre-processing textual data is a standard task. Several Python libraries including `scikit-learn` and `Keras` offer similar functionality. We will use the `NLTK toolkit` in this notebook. It has a clear and easy to understand syntax and is well-suited to demonstrate standard NLP operations. Although not the focus of this tutorial, we also introduce a library called `Beautiful Soup`, which gained a lot of popularity in web-scraping. Make sure to have these libraries installed before running the following codes. \n",
    "Here is the agenda of the session:\n",
    "\n",
    "1. Preparing text for analysis: the standard NLP pipeline\n",
    "2. Use case: the IMDB movie review data set\n",
    "3. Demo: training word-to-vec word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hB9cN44lA74V",
    "outputId": "bb5bc01d-d8b2-4503-fe91-13b72a055054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Library for standard NLP workflow\n",
    "import nltk  # When running this notebook for the first time, you have to download the following NLTK packages. Just uncomment the next 3 lines\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft9tayXsA74X"
   },
   "source": [
    "### 1. Preparing text for analysis: the standard NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0JQocRpsA74Z",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Just some demo text; no need to quote I guess :)\n",
    "text_raw = \"\"\" \n",
    "            I wonder if I have been changed in the night. Let me think. Was I the same when I got up this morning? \n",
    "            I almost can remember feeling a little different. But if I am not the same, the next question is 'Who in the world am I?' \n",
    "            Ah, that is the great puzzle!\n",
    "           \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WglFCITTTmXk"
   },
   "source": [
    "#### Remove Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "PTrQd1jjTqd6",
    "outputId": "fc8abb29-f51f-4589-ade2-f7010f67cb35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I wonder if I have been changed in the night. Let me think. Was I the same when I got up this morning? I almost can remember feeling a little different. But if I am not the same, the next question is 'Who in the world am I?' Ah, that is the great puzzle!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" Function to remove whitespace (tabs, newlines). \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "text_processed = remove_whitespace(text_raw)\n",
    "text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ZFX6nlT5q9"
   },
   "source": [
    "Hm, but the punctuation is there still. Is it noise or is it useful? Let's try removing it for now (there is a bunch of methods out there). Additionally we will drop weird symbols and lower the big cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUPvGuL2A74g"
   },
   "source": [
    "#### Punctuation, Whitespace and Casing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "xX10anQDEYlm",
    "outputId": "e89bd6bf-f3c2-4d84-8edc-731cc7a64379"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'i wonder if i have been changed in the night let me think was i the same when i got up this morning i almost can remember feeling a little different but if i am not the same the next question is in the world am i ah that is the great puzzle'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation_and_casing(text):\n",
    "    \"\"\"\n",
    "    Function to remove the punctuation, upper casing and words that include\n",
    "    non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    chars = '!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    text = text.translate(str.maketrans(chars, ' ' * len(chars)))\n",
    "    return ' '.join([word.lower() for word in text.split() if word.isalpha()])\n",
    "\n",
    "text_processed = remove_punctuation_and_casing(text_processed)\n",
    "text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOaPEAuCA74i"
   },
   "source": [
    "This is starting to look like a dictionary already, right? There are some more issues we want to address though. Like 'stop words' - semantically they do not mean much but serve to put sentences together (\"the\", \"a\", \"and\", etc) - they will add noise. NLTK can offer you its own list of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mZqEl0NA74k"
   },
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ0OQxuPMz3i",
    "outputId": "08dfcb1f-15b6-4b94-8bad-697636864557"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv4tYmvGNlKh",
    "outputId": "10387c80-907f-45aa-bd6f-f127d9ca8579"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG8Rf-TFA74n"
   },
   "source": [
    "The list of stop words looks comprehensive. However, say you miss a 'stop word' that you would also like to filter. You can extend the above list easily. After all, it is just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y58lHR8HA74p",
    "outputId": "8f7e8491-62b9-4e55-c829-0af7fff72697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of stopwords is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "t = type(english_stopwords)\n",
    "print('Data type of stopwords is:', t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "WMAbMUW0A74q",
    "outputId": "2e4340a0-ca19-4126-9d6f-e55334d03a5a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'some_word_you_dont_like'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add some custom stopwords\n",
    "english_stopwords.append('some_word_you_dont_like')  # you can apply all the functions for lists\n",
    "english_stopwords[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZJ3fUEPNt_q"
   },
   "source": [
    "Finally, let's remove the stopwords from our processed sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "D775Y7WhNegS",
    "outputId": "6590ff68-740a-4ef9-ae24-361bd2024cb1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wonder changed night let think got morning almost remember feeling little different next question world ah great puzzle'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" Function to remove stopwords. \"\"\"\n",
    "    return ' '.join([word for word in str(text).split() if word not in english_stopwords])\n",
    "\n",
    "text_processed = remove_stopwords(text_processed)\n",
    "text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ON3INRoA74v"
   },
   "source": [
    "#### Lemmatization and Stemming\n",
    "You might have already thought of the issue: what if a word is used in different forms? It will be treated as different words semantically right? That is where **stemming** and **lemmatization** comes into play. The former approach is simpler and consists mainly of dropping suffices. The later reduces a word to its dictionary form. To that end, we need to have a dictionary available. Let's first illustrate simple stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "mO2xMJs-OUAn",
    "outputId": "faff4442-bb9a-4c4f-ab2c-2a55f34a2f3c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wonder chang night let think got morn almost rememb feel littl differ next question world ah great puzzl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the sake of completness, here is an example for using stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\" Function to stem words. \"\"\"\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "text_processed = stem_words(text_processed)\n",
    "text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuHgXVBOA74y"
   },
   "source": [
    "Simple, isn't it. With just one example sentence, it is hard to appreciate the benefits of stemming. The idea is that if we have a large corpus many words will appear multiple times in different grammatical forms. Still, the meaning that these words carry is roughly identical. Running, run, ran, runner, etc. all of these words indicate that the text has something to do with running. Assuming that this is all we need to know -- yes that is a bold assumption -- stemming makes sense as it could greatly reduce the number of distinct words in a corpus. This number of distinct words, also called **vocabulary size**, is very important. It effects the efficiency of NLP operations and may also have a big impact on the accuracy of text classification. <br>\n",
    "Let's now take a look on lemmatization. Here, things are a little more complicated. While NLTK offers a ready-to-use function, we need to tell it the grammatical form of the word that we want to lemmatize. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQhYXvf1A741",
    "outputId": "2cea0e34-962d-456e-9e09-e2a703364485",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripe\n",
      "strip\n"
     ]
    }
   ],
   "source": [
    "# NLTK lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# You need to choose the type of word:\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v')) # what happens if we claim this is a verb? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR5BVbzeA743"
   },
   "source": [
    "How would we know that grammatical form? In fact, determining this form is an NLP task in its own right. It is called **POS tagging**. Much research has been done on coming up with clever ways to determine POS (part-of-speech) tags. We will not go into details. A simple POS tagger is available as part of the `NLTK` library. it can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZ8B969jA743",
    "outputId": "1b545433-9133-4143-b43e-f69f35d1f235",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('earned', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('stripes', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('great', 'JJ'),\n",
       " ('performance', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"She\", \"earned\", \"her\", \"stripes\", \"with\", \"great\", \"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0YTMpiOA744"
   },
   "source": [
    "We make use of the above POS tagger later. For now, let's simply use the lemmatizer to reduce words to their dictionary form, irrespective of part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "O8mb4ZdDPnO0",
    "outputId": "38e4cdcd-b0b7-4773-81a0-43ead10286ae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wonder chang night let think got morn almost rememb feel littl differ next question world ah great puzzl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_words(text, **kwargs):\n",
    "    \"\"\" Function to lemmatize words. \"\"\"\n",
    "    return ' '.join([lemmatizer.lemmatize(word, **kwargs) for word in text.split()])\n",
    "\n",
    "text_processed = lemmatize_words(text_processed)\n",
    "text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX1GEsTJA745"
   },
   "source": [
    "#### Cleaning HTML\n",
    "\n",
    "For a more sophisticated cleaning of text, you might want to consider **regular expressions**. In a nutshell, regular expressions are a family of text processing techniques for searching and replacing text. Their capability to match expressions in a text, for example an email, is quite powerful. A quick read through the corresponding [Wikipedia page](https://en.wikipedia.org/wiki/Regular_expression) would be useful. Also, here is a [nice playground](https://regexr.com/). Using a regular expression, we could re-write the above code as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1HvWJK1A746"
   },
   "source": [
    "This text includes the email address of Stefan <stefan.lessmann@hu-berlin.de>. \n",
    "Also, we use <em>html</em> to <b>emphasize</b> parts and include breaks <br> to separate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yWF2XQHcA746"
   },
   "outputs": [],
   "source": [
    "# Another piece of demo text illustrating some common issues\n",
    "re_demo = \"\"\"\n",
    "            This text includes the email address of Stefan <stefan.lessmann@hu-berlin.de>. \n",
    "            Also, we use <em>html</em> to <b>emphasize</b> parts and include breaks <br> to separate lines.\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm80_WmlA747"
   },
   "source": [
    "Finding or filtering email addresses is a common use case when processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "iOsPzg9aA748",
    "outputId": "229cd2be-0354-44e7-8dd1-d0bae73a4d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  ['stefan.lessmann@hu-berlin.de']\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This text includes the email address of Stefan <>. Also, we use <em>html</em> to <b>emphasize</b> parts and include breaks <br> to separate lines.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding emails using RE\n",
    "import re  # Python library for regular expressions\n",
    "\n",
    "# Simple pattern to match email addresses\n",
    "pat = '([\\w\\.-]+@[\\w\\.-]+\\.[\\w]+)+'\n",
    "\n",
    "# Extracting email addresses\n",
    "email = re.findall(pat, re_demo)\n",
    "print('Found: ', email)\n",
    "\n",
    "# Filter sub-strings\n",
    "re.sub(pat, '', remove_whitespace(re_demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "_HcSgtGQA749",
    "outputId": "426503a4-14af-4eb9-f537-257dc55a9171"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This text includes the email address of Stefan . Also, we use html to emphasize parts and include breaks to separate lines.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library beatifulsoup4 handles html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove html content\n",
    "remove_whitespace(BeautifulSoup(re_demo).get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSn3xjDQVVGI"
   },
   "source": [
    "#### Conversion of Emojis and Emoticons\n",
    "\n",
    "Emoticons and emojis are a sequence of ASCII characters or unicode images that express moods or feelings in written communication. In use cases like sentiment analysis, emoticons and emojis give very valuable information.\n",
    "\n",
    "One way to make use of the information is to convert the emoticons and emojis into text that reflects their meaning. For that, we will be using a condensed copy of the \"emot\" library by Neel Shah [(Github)](https://github.com/NeelShah18/emot/blob/master/emot/core.py). When running this notebook, make sure that the file ```emot_dictionary.py``` is in the same directory as the notebook. Note, that when using Google Colab it may be simpler to install the \"emot\" package directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "utMMkwg6WanO"
   },
   "outputs": [],
   "source": [
    "import emot_dictionary as emot\n",
    "\n",
    "emo_demo = \"\"\"\n",
    "            The movie was fantastic :o :-)) ðŸš€ ðŸ‘\n",
    "           \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJqeXfmYjYfJ"
   },
   "source": [
    "Let's first convert the emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "6AmM56Q8UXtY",
    "outputId": "f32c6032-b064-4bed-fc6d-bfb301e3a5ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The movie was fantastic Surprise Very happy ðŸš€ ðŸ‘'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emoticons(text):\n",
    "    \"\"\" Function to convert emoticons into a text that reflects their meaning. \"\"\"\n",
    "    EMOTICONS = emot.EMOTICONS()\n",
    "    for i in EMOTICONS:\n",
    "        text = text.replace(i, EMOTICONS[i])\n",
    "    return text\n",
    "\n",
    "emo_demo = convert_emoticons(remove_whitespace(emo_demo))\n",
    "emo_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-kf6B_sjblj"
   },
   "source": [
    "And now we also convert the emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "tXOHRTaQY2hI",
    "outputId": "60c0842c-4c1e-4183-e9a5-2407af8d0f09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The movie was fantastic Surprise Very happy rocket clapping hands'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis(text):\n",
    "    \"\"\" Function to convert emojis into a text that reflects their meaning. \"\"\"\n",
    "    EMOJIS = emot.EMOJIS_UNICODE()\n",
    "    for i in EMOJIS:\n",
    "        text = text.replace(EMOJIS[i], i.translate(str.maketrans('', '', ':')).replace(r'_', r' '))\n",
    "    return text\n",
    "\n",
    "convert_emojis(emo_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlDAJAcKA74-"
   },
   "source": [
    "# Wrapping up\n",
    "Albeit simple, the above demos provide a glance on text cleaning. While you could do a lot more, tasks like stop word removal, etc. will come up in many NLP projects. We conclude this part by putting all of the above steps into a helper function, which we will use later in the session to clean a data set of online movie reviews. Our helper function will use lemmatization instead of stemming because it is likely to give better results in downstream tasks (i.e., text classification). The following function is yet another helper to call the lemmatizer with the right dictionary form of a word.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zy-F92DLA75C"
   },
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Helper function that calls the POS tagger for an input word and return a code that can be used for lemmatization\"\"\"\n",
    "    # Extract the first letter of the POS tag (see the above example to understand the output coming from pos_tag)\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  \n",
    "    # Dictionary to map these letters to wordnet codes that the lemmatizer understands\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8EcGqdoA75C",
    "outputId": "380234fd-5c83-471c-9554-247abdd9d7fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'v', 'n', 'n', 'n', 'a', 'n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function\n",
    "[get_wordnet_pos(x) for x in [\"She\", \"earned\", \"her\", \"stripes\", \"with\", \"great\", \"performance\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ljzZyj2A75D"
   },
   "source": [
    "And here is the real helper function for text cleaning. We will make use of it right after introducing our data set for subsequent parts. Since that data is stored in the form of a data frame, we refrain from making our helper function more general and simply assume that incoming text is a Pandas Series object (i.e., one column of a data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "D1yH62ujlemE"
   },
   "outputs": [],
   "source": [
    "def text_cleaning(documents):\n",
    "    \"\"\"\n",
    "    Function for standard NLP pre-processing including removal of html tags,\n",
    "    whitespaces, non-alphanumeric characters, and stopwords. Emoticons are\n",
    "    converted to text that reflects their meaning. Words are subject to\n",
    "    lemmatization using their POS tags.\n",
    "    \"\"\"\n",
    "    cleaned_text = []  # our output will be a list of documents\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print('Processing input array with {} elements...'.format(documents.shape[0]))\n",
    "    counter = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        text = BeautifulSoup(doc).get_text() # remove html content\n",
    "        text = remove_whitespace(text) # remove whitespaces\n",
    "        text = convert_emoticons(text) # convert emoticons to text\n",
    "        text = remove_punctuation_and_casing(text) # remove punctuation and casing\n",
    "        text = remove_stopwords(text) # remove stopwords\n",
    "        text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()]) # lemmatize each word\n",
    "        \n",
    "        cleaned_text.append(text)\n",
    "\n",
    "        if (counter > 0 and counter % 50 == 0):\n",
    "            print('Processed {} documents'.format(counter))\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADG0dtq1A75E"
   },
   "source": [
    "## 2. Use case: the IMDB movie review data set\n",
    "We use a popular NLP data set consisting of movie reviews posted at [IMDB](https://www.imdb.com/). The data is available in different sizes and shapes (cleaned, raw, ...) on the web. We use a version from Kaggle, which includes 50K reviews and binary labels whether a review is positive or negative. The labels are useful for sentiment analysis, which we will do in our next programming session. Here, we simply prepare the data for the next session and thereby further elaborate on the NLP operations introduced in the previous part. You can download the raw data from Kaggle: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data. A version is also available in the course folder corresponding to Block II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z3LFVVCA75F"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W632-T2fA75F",
    "outputId": "aca3a448-c685-4685-804e-514d013bdf5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remeber to adjust the path so that it matches your environment\n",
    "import pandas as pd\n",
    "\n",
    "imdb_data = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xa8Oiw5bA75G",
    "outputId": "239db651-e58b-4e75-9f18-56f1a34eb7fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b3de7b3a-9586-4a47-b9d1-010edabc633c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3de7b3a-9586-4a47-b9d1-010edabc633c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b3de7b3a-9586-4a47-b9d1-010edabc633c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b3de7b3a-9586-4a47-b9d1-010edabc633c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQGcjRNEA75G"
   },
   "source": [
    "So the data is really simple; just two columns, one for the binary sentiment and one for the text of the review. Apparently, some of the reviews include HTML. We already added functionality to handle HTML into our text cleaning function. So this should not cause us any trouble. Let's look at an arbitrary review to get a better understanding of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "nO7pNaz_A75H",
    "outputId": "91089463-a725-4654-c4a8-c14ee82a21b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.loc[8, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "76meDV-dA75H",
    "outputId": "04a418ab-d84d-476c-9298-22aae0669769"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.loc[8, 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FwuSSJjA75I"
   },
   "source": [
    "### Sampling\n",
    "Working with the full data set of 50K reviews is time consuming. When experimenting with the notebook, you might want to draw a random sample to increase the speed of computations. For a modern computer, a sample size of 5000 should be feasible without increasing the time too much. Note that results of processing the full data sets are available in our course folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSZlYQpCA75I",
    "outputId": "58a4603b-7ade-461f-df89-b912c7b4482d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     500 non-null    object\n",
      " 1   sentiment  500 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Draw a radnom sample to save time\n",
    "sample_size = 500\n",
    "np.random.seed(111)\n",
    "idx = np.random.randint(low=0, high=imdb_data.shape[0], size=sample_size)\n",
    "imdb_data = imdb_data.loc[idx,:]\n",
    "\n",
    "imdb_data.reset_index(inplace=True, drop=True)  # dropping the index prohibits a reidentification of the cases in the original data frame\n",
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYum-n2lA75K"
   },
   "source": [
    "### Data cleaning\n",
    "Thanks to our careful preparation, cleaning the reviews should be easy. All it takes is applying our cleaning function to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6lIb9J4rdnD",
    "outputId": "4e1d1d40-96c6-4d7b-f9d3-ceb7abbcfd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input array with 500 elements...\n",
      "Processed 50 documents\n",
      "Processed 100 documents\n",
      "Processed 150 documents\n",
      "Processed 200 documents\n",
      "Processed 250 documents\n",
      "Processed 300 documents\n",
      "Processed 350 documents\n",
      "Processed 400 documents\n",
      "Processed 450 documents\n",
      "Duration: 8 sec\n"
     ]
    }
   ],
   "source": [
    "# Do the cleaning\n",
    "# CAUTION: depending on your data set size, the processing might take a while \n",
    "import time  # To keep an eye on runtimes\n",
    "start = time.time()\n",
    "imdb_data['review_clean'] = text_cleaning(imdb_data.review)\n",
    "print('Duration: {:.0f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kyrIcWSwA75M",
    "outputId": "71d390cf-203c-4564-eaac-4b1ada6359b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      "One of the best horror/suspense movies I have seen in a long time. Wow, it was a big surprise and stunning at how good this movie was, sometimes a gem like this will surface but is rare. I expected a popcorn monster flick and a mildly diverting way to spend a late night but instead a very well made and directed movie with great acting and made with passion and heart. <br /><br />This is a movie that makes you feel for the characters and what happens to them, and it is filmed like you are there and it is really happening. I know some people in other reviews compare it to \"Open Water\", but I disagree because I thought Open Water was quite boring and mediocre, while this movie was the opposite, although superficially they are filmed in the same \"realistic\" style.<br /><br />The actors are unknowns, at least to me, but they all are very effective and convey the dire situation with frightening intensity and realism. The story is well done and flows smoothly, the plot is logical and appears to be something that could happen, all the actions and thoughts of the characters are quite what a person would do and think about. Very believable and this makes the movie more real because of it.<br /><br />I had tears in my eyes at the end. I must say a movie seldom has this effect on me, this is how powerful and emotional this movie was done and I am suitably impressed by the director and actors of this great movie.\n",
      "\n",
      "Cleaned Review:\n",
      "one best horror suspense movie see long time wow big surprise stun good movie sometimes gem like surface rare expect popcorn monster flick mildly divert way spend late night instead well make direct movie great act make passion heart movie make feel character happens film like really happen know people review compare open water disagree thought open water quite boring mediocre movie opposite although superficially film realistic style actor unknown least effective convey dire situation frighten intensity realism story well do flow smoothly plot logical appear something could happen action thought character quite person would think believable make movie real tear eye end must say movie seldom effect powerful emotional movie do suitably impressed director actor great movie\n"
     ]
    }
   ],
   "source": [
    "# Check all is well\n",
    "print('Original Review:\\n' + imdb_data.review[46])\n",
    "print('\\nCleaned Review:\\n' + imdb_data.review_clean[46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAeDBflKvbm7",
    "outputId": "21cf7211-d77b-49fc-ecbe-8efcbe29d4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   review        500 non-null    object\n",
      " 1   sentiment     500 non-null    object\n",
      " 2   review_clean  500 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "mbL-YTjPr6pv",
    "outputId": "a94280a3-55cd-4b8d-fe73-0cc570f8fb8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-43c1501c-3c71-42f3-b9db-04240d70a5a0\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The script seems to have been wholesale (ahem ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>script seem wholesale ahem ahem cough cough bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I watched this again after having not seen it ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>watch see since first come still make laugh lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ultimately too silly and pointless. Yes there ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ultimately silly pointless yes gild cage metap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice attempt and good ideas (redemption of the...</td>\n",
       "      <td>negative</td>\n",
       "      <td>nice attempt good idea redemption prostitute h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A touching love story reminiscent of Ã‚Â‘In the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>touch love story reminiscent mood draw heavily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Went to watch this movie expecting a 'nothing ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>go watch movie expect really action flick stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Rickman &amp; Emma Thompson give good perform...</td>\n",
       "      <td>negative</td>\n",
       "      <td>alan rickman emma thompson give good performan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The only reason I give it a 2 is that filmogra...</td>\n",
       "      <td>negative</td>\n",
       "      <td>reason give filmography stylize day least some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43c1501c-3c71-42f3-b9db-04240d70a5a0')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-43c1501c-3c71-42f3-b9db-04240d70a5a0 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-43c1501c-3c71-42f3-b9db-04240d70a5a0');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  The script seems to have been wholesale (ahem ...  negative   \n",
       "1  I watched this again after having not seen it ...  positive   \n",
       "2  Ultimately too silly and pointless. Yes there ...  negative   \n",
       "3  Nice attempt and good ideas (redemption of the...  negative   \n",
       "4  A touching love story reminiscent of Ã‚Â‘In the ...  positive   \n",
       "5  Went to watch this movie expecting a 'nothing ...  negative   \n",
       "6  Alan Rickman & Emma Thompson give good perform...  negative   \n",
       "7  The only reason I give it a 2 is that filmogra...  negative   \n",
       "\n",
       "                                        review_clean  \n",
       "0  script seem wholesale ahem ahem cough cough bo...  \n",
       "1  watch see since first come still make laugh lo...  \n",
       "2  ultimately silly pointless yes gild cage metap...  \n",
       "3  nice attempt good idea redemption prostitute h...  \n",
       "4  touch love story reminiscent mood draw heavily...  \n",
       "5  go watch movie expect really action flick stil...  \n",
       "6  alan rickman emma thompson give good performan...  \n",
       "7  reason give filmography stylize day least some...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFkavtnmA75M"
   },
   "source": [
    "Looks like the cleaning has fulfilled its purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5_nwC3bA75O"
   },
   "source": [
    "### File input and output\n",
    "Should you have used the full data set in the above cleaning, you will want to store your results. The following codes exemplifies the use of a library called `Pickle`, which Pandas support natively to store data sets in a binary format. Compared to csv, the advantage of a binary format is that the data needs less space on disk. Note that you might have to install `Pickle` for the code to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "92n7hUs-A75Q"
   },
   "outputs": [],
   "source": [
    "# Saving objects to disk using pickle\n",
    "import pickle\n",
    "\n",
    "imdb_data.to_pickle('imdb_full_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV8EPJzWA75Q"
   },
   "source": [
    "### A bird's eye view on the data\n",
    "Let's have a quick look at what folks talk about in this data set. Using the class *Counter* from the collections package, we can easily count word occurrences and query the most common words. We can also check the number of occurrences for specific words. We do not really need the *word_counter* here and only use it to get a feeling for the data set. Our course, these types of checks make more sense when using the full data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNPKVlhSA75R",
    "outputId": "d4bf1dca-e1f4-4d93-b77e-ea6c1d3ad128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a bit of code to load the data of the clean reviews from our course folder\n",
    "import pickle\n",
    "with open('imdb_clean_full.pkl','rb') as path_name:\n",
    "    clean_reviews = pickle.load(path_name)\n",
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "2toK51cXA75R"
   },
   "outputs": [],
   "source": [
    "# Loop through the words and update a counter keeping track of word counts\n",
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for r in clean_reviews:\n",
    "    for w in r:\n",
    "        word_counter.update({w: 1})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9oiRYXCA75S",
    "outputId": "a08dfb81-bcac-4345-96d3-f956761a0149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 103239),\n",
       " ('film', 95848),\n",
       " ('one', 55432),\n",
       " ('make', 46127),\n",
       " ('like', 44297),\n",
       " ('see', 41611),\n",
       " ('get', 35196),\n",
       " ('well', 32874),\n",
       " ('time', 31548),\n",
       " ('good', 29852)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the top most frequent words\n",
    "top_n = 10\n",
    "word_counter.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1dInKqlA75T"
   },
   "source": [
    "The above results hints at some more challenges when working with text data. Among the top ten most frequent words, none is really surprising or appears interesting. Well, what is interesting depends on the task. For example, words like *like* and *good* have meaning in a sentiment analysis setting. However, words like *movie* or *film* will naturally appear in a data set on movie reviews and do not reveal much information for sentiment analysis. This indicates that, in addition to filtering stop words, there could be other 'normal' words (i.e., not stop words) that we might want to filter. Again, preparing text data can be rather laborious...<br>\n",
    "Let's check if people also talk about something more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u87DFzCDA75U",
    "outputId": "ca56517c-e0c7-4ba7-d821-fb0122446c91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check frequency of some target word\n",
    "word_counter[\"spielberg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdTgrnLmA75V"
   },
   "source": [
    "## 3. Demo: training word-to-vec word embeddings\n",
    "Strictly speaking, the session is over. We covered the fundamental steps in a NLP pre-processing pipeline. You can easily skip the remainder of the notebook, in which we demonstrate one way to train word vectors using word-to-vec (W2V). When it comes to embeddings, the most typical use case is to **download pre-trained embeddings** and employ these for some downstream tasks (with or without fine-tuning). Keras supports that use case very well, as we will see in the next session. At times, however, you may want to **train your own embeddings**. For example, our data is from IMDB. People talk about movies. Maybe they use a specific type of language that is not well represented in pre-trained W2V embeddings. Without going into too much detail of the pros and cons of pre-training your own word embeddings, the point of the following codes is simply to showcase how you can do it if you want to. We will use a library called `Gensim`.\n",
    "\n",
    "`Gensim` is a popular library for text processing. Although maybe even more geared toward topic modeling, it offers, amongst others, implementations of several algorithms to learn word embeddings including *W2V*, *GloVe*, and *Fasttext*. The following demonstrates training W2V embeddings using the cleaned reviews. Before moving on, you might need to install `Gensim`  ;)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liLqBv7CA75V"
   },
   "source": [
    "### The Gensim W2V model\n",
    "Training word embeddings using `Gensim` is very easy and just a matter of calling a function. Well, the reason it takes so little code is that we have already cleaned our data and have it available as an array of texts; that is a format that `Gensim`supports. However, note that, depending on your data, the code may take quite a while to run. Again, word embeddings trained on the full 50K data set for 500 epochs are available in our course folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "HN4TnV8zA75W"
   },
   "outputs": [],
   "source": [
    "# CAUTION: Running the code might take a while\n",
    "from gensim.models import Word2Vec    \n",
    "\n",
    "emb_dim = 50  # embedding dimension\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(clean_reviews, \n",
    "                 min_count=1,  #min_count means the frequency benchmark, if =2 and word is used only once - it's not included\n",
    "                 window=5,     #the size of context\n",
    "                 iter=10,      #how many times the training code will run through the data set, same as epochs. The first pass is to create dict. Set to >200 to obtain reasonable results\n",
    "                 size=emb_dim, #size of embedding\n",
    "                 workers=2)    #for parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAMd9pS0A75X",
    "outputId": "68918eeb-16f2-45b8-dda0-9db671952730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=84673, size=50, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.26228845,  0.34722286,  1.4709738 ,  2.0605502 , -2.2747583 ,\n",
       "       -3.351172  , -0.93709004,  0.30147228,  0.3833407 , -0.93977964,\n",
       "       -3.5641367 ,  1.5985901 , -2.761732  , -0.8686265 ,  1.0060619 ,\n",
       "       -2.178143  ,  1.3885053 , -0.72174406, -1.8205775 ,  0.76048607,\n",
       "       -0.20302485,  1.9258785 ,  0.85206515, -1.9027067 ,  1.0411849 ,\n",
       "        0.5764043 ,  1.5255504 , -2.4159296 ,  1.9932901 , -1.7513642 ,\n",
       "       -0.5735383 , -0.02365238, -1.5335919 , -0.777492  , -1.1198854 ,\n",
       "       -0.41005844,  0.9909722 ,  0.04843368,  1.2031087 ,  1.5734948 ,\n",
       "       -0.14604816, -1.801444  ,  1.5539323 , -0.2381962 , -2.5179162 ,\n",
       "       -1.8608973 , -0.6056832 ,  1.7911577 ,  2.10289   ,  2.5480464 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "print(model)\n",
    "words=list(model.wv.vocab)\n",
    "\n",
    "# get one embedding \n",
    "model.wv['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKjhdPPAA75X"
   },
   "source": [
    "### Input / output handling\n",
    "Gensim supports saving and loading of trained embeddings in different versions. Below is a simple demo. More information is available on the [Gensim homepage](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Bo93egFsA75Y"
   },
   "outputs": [],
   "source": [
    "# Save the trained word vectors\n",
    "file=\"w2v_embedding.model\"\n",
    "save_as_bin = False\n",
    "model.wv.save_word2vec_format(file, binary=save_as_bin)  # set binary to True to save disk space; false facilitates inspecting the embeddings in a text editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G99ww5tA75Y"
   },
   "source": [
    "### Working with the trained embeddings\n",
    "We use a pre-trained version of the embeddings, which were trained on the full IMDB data set for 500 epochs. The examples are inspired by [this Kaggle kernel](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial). If you want to visualize the trained word vectors have a look at [this post](https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne). It is fairly easy to create a visualization but to get meaningful results you would need to prepare the data more carefully; for example removing too frequent words and too infrequent words. Ultimately, working with the embeddings would involve building a proper NLP model and using it to solve some downstream tasks, as we will do in the next session. The following codes simply showcase the embeddings and functionality of `Gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "eD7dCexjA75Z"
   },
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "file = \"w2v_imdb_dim50_embeddings.model\"\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYt70SnBA75Z"
   },
   "source": [
    "#### Which word is most similar to another word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lewhU1G7A75a",
    "outputId": "ab90e32d-b95e-4f35-ef74-3b7ff671729b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mcnaughton', 0.5568448901176453),\n",
       " ('gjs', 0.552051305770874),\n",
       " ('haht', 0.5311591029167175),\n",
       " ('furia', 0.529901921749115),\n",
       " ('shawniqua', 0.5293144583702087),\n",
       " ('emanate', 0.5260404348373413),\n",
       " ('vallee', 0.5185524821281433),\n",
       " ('imagineered', 0.5167945623397827),\n",
       " ('jingle', 0.5123077034950256),\n",
       " ('damir', 0.5118454694747925)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['avatar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v4DJ1egA75b"
   },
   "source": [
    "#### How similar are two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P22xPLE5A75c",
    "outputId": "ed2f54db-b147-4e8d-9036-aee3b1c3e5c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7539246"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('good', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5pxaVl8A75c",
    "outputId": "3775a2eb-3a1b-47ad-b06b-091fe28615a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How similar is Tarantino to Spielberg: 0.5265995860099792\n",
      "How similar is Emmerich to Spielberg: 0.20405761897563934\n",
      "How similar is Paltrow to Bullock: 0.20976237952709198\n",
      "How similar is Paltrow to Alba: 0.17420797049999237\n",
      "How similar is Cruise to Depp: 0.16261133551597595\n",
      "How similar is Cruise to Willis: -0.04004356637597084\n"
     ]
    }
   ],
   "source": [
    "print('How similar is Tarantino to Spielberg: {}'.format(model.similarity('tarantino', 'spielberg')))\n",
    "print('How similar is Emmerich to Spielberg: {}'.format(model.similarity('emmerich', 'spielberg')))\n",
    "\n",
    "print('How similar is Paltrow to Bullock: {}'.format(model.similarity('paltrow', 'bullock')))\n",
    "print('How similar is Paltrow to Alba: {}'.format(model.similarity('paltrow', 'alba')))\n",
    "\n",
    "print('How similar is Cruise to Depp: {}'.format(model.similarity('cruise', 'depp')))\n",
    "print('How similar is Cruise to Willis: {}'.format(model.similarity('cruise', 'willis')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxlk22MJA75d"
   },
   "source": [
    "#### Which word does not fit in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "JPYGN35DA75d",
    "outputId": "b974bffc-2b12-4523-fba7-2567de668023"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['cool', 'great', 'lovely', 'weak'])\n",
    "model.doesnt_match(['movie', 'film', 'good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htZvGR4YA75e"
   },
   "source": [
    "#### A is to B as C is to ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-DTdgxNA75e",
    "outputId": "6621430d-aafd-42a6-c198-51eabc845099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indefinably', 0.5315033793449402),\n",
       " ('directorial', 0.5310426950454712),\n",
       " ('grrrrrrr', 0.5281748175621033),\n",
       " ('tarrentino', 0.5250871777534485),\n",
       " ('meshugaas', 0.5238608717918396)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['spielberg', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeiOWgJkA75h"
   },
   "source": [
    "### Phrase detection\n",
    "W2V trains one embedding per word. The model is agnostic of common phrases such as 'New York'. It would train one embedding for new and another for york, provided both words are part of the vocabulary. You can get better embeddings by adding common phrases to the vocabulary. W2V will then train individual embeddings for these phrases. Gensims also comes with a phrase detection models, which allows you to handle bigrams, trigrams and the like. We will not retrain our W2V model but sketch how you can use Gensim to get these common phrases. You could then consider to add (some of) them to your vocab and enhance the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "JgFV1l0SA75h"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Train a bigram model\n",
    "bigram_model = Phrases(clean_reviews, min_count=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdmEVqvdA75h"
   },
   "source": [
    "After training, we can take text and put it through the bigram model. The model will then alter the text so as to introduce bigrams. Here is an example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzZUbniKA75h",
    "outputId": "0edab39a-866b-4f3a-fcd0-5968295d2bb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewer_mention',\n",
       " 'watch',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model[clean_reviews[0][0:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f-NZA7wA75i",
    "outputId": "34657188-58e2-4fe0-980c-2f3550f9d111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewer',\n",
       " 'mention',\n",
       " 'watch',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to the original review\n",
    "clean_reviews[0][0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGbE6LWEA75j"
   },
   "source": [
    "We can also make use of our counter class to examine the most common bigrams in the corpus, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "JefH9K6TA75j"
   },
   "outputs": [],
   "source": [
    "bigram_counter = collections.Counter()\n",
    "for key in bigram_model.vocab.keys():\n",
    "    if key.decode().find('_')>-1: # the decode is needed because Gensims stores keys as bytes\n",
    "        bigram_counter[key] += bigram_model.vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72LnWyn2A75k",
    "outputId": "8d0cc7c8-6c26-4ef3-a7a1-6707798e8631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'look_like', 3719),\n",
       " (b'watch_movie', 3109),\n",
       " (b'ever_see', 2977),\n",
       " (b'see_movie', 2746),\n",
       " (b'bad_movie', 2698),\n",
       " (b'year_old', 2422),\n",
       " (b'film_make', 2369),\n",
       " (b'make_movie', 2359),\n",
       " (b'special_effect', 2324),\n",
       " (b'movie_make', 2132),\n",
       " (b'even_though', 2000),\n",
       " (b'movie_ever', 1987),\n",
       " (b'main_character', 1924),\n",
       " (b'one_best', 1921),\n",
       " (b'movie_like', 1919),\n",
       " (b'low_budget', 1896),\n",
       " (b'make_film', 1878),\n",
       " (b'see_film', 1843),\n",
       " (b'waste_time', 1775),\n",
       " (b'watch_film', 1652),\n",
       " (b'good_movie', 1618),\n",
       " (b'horror_movie', 1612),\n",
       " (b'much_well', 1533),\n",
       " (b'want_see', 1485),\n",
       " (b'seem_like', 1473)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKzSOo6NA75k"
   },
   "source": [
    "The above bigrams might be frequent. However, you would not consider training individual embeddings for phrases such as *look_like* or *waste_time*. This shows how proper phrase detection in the scope of W2V is nontrivial and would require more work before we can hope to get descend results.     "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P.II.2_nlp_foundations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
