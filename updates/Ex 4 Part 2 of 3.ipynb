{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #4: Neural Networking (NN) Primer\n",
    "## Part 2 of 3\n",
    "This is the second of three notebooks which will cover the foundations of neural networks. Recall that every connection in a neural network carries a weight. Therefore, calculating the *forward pass* requires concrete values for these weights. In the first notebook, we discussed how the weights can be initialized randomly. The focus of this notebook is network training. Training is the process in which we update the weights to maximize the fit of our network to (the training) data. Network training is, therefore, equivalent to maximum likelihood estimation in regression analysis. \n",
    "\n",
    "## This notebook's topic: back propagation in neural networks ##\n",
    " 1. Overview of the forward pass\n",
    " 2. Caculating loss\n",
    " 2. Finding a minimum\n",
    " 3. Moving towards a minimum\n",
    " 4. One solver for gradient descent: Stochastic gradient descent\n",
    " 5. Stopping Rules\n",
    " 6. Overview of back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(888)  # set seed for reproducibility (numPy functions)\n",
    "random.seed(888)  # set seed for reproducibility (random package functions)\n",
    "\n",
    "range_for_demo = np.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "n = 1000  # number of observations in our simulation\n",
    "k = 15    # number of features in X in our simulation\n",
    "\n",
    "XX, Y = make_regression(n_samples=n, n_features=k, noise=5, random_state=888)\n",
    "\n",
    "Y = Y.reshape(n, 1)  # Make sure that y is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we revisit the forward pass through a neural network. Here, we simply go through the corresponding code. The first part of this series of notebooks explains the steps in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture:\n",
    "inputLayer_size = k      # number of features in X\n",
    "hiddenLayer_size = 10    # number of hidden layer nodes\n",
    "outputLayer_size = 1     # number of values to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization for first forward pass\n",
    "\n",
    "limit = np.sqrt(6 / (inputLayer_size + outputLayer_size))  # Recommended weight initialization\n",
    "\n",
    "W_0 = np.random.uniform(-limit, limit, (hiddenLayer_size, inputLayer_size))    # Random weight initialization\n",
    "W_1 = np.random.uniform(-limit, limit, (outputLayer_size, hiddenLayer_size))  # Random weight initialization\n",
    "\n",
    "B_0  = np.ones((hiddenLayer_size,1))  # Bias initialization\n",
    "B_1 = np.ones((outputLayer_size,1))  # Bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function that we will use in this scenario and its derivative\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)  # 0 if input is negative, x if input is positive\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First forward pass\n",
    "X = np.array(XX[5]).reshape((inputLayer_size, 1))  # observation 5\n",
    "y = Y[5].reshape((outputLayer_size, 1))  # true value of target\n",
    "Z_hidden = np.dot(W_0, X) + B_0  # first hidden layer inputs\n",
    "H = ReLU(Z_hidden)  # hidden layer output (after activation)\n",
    "Z_output = np.dot(W_1, H) + B_1  # input to output layer\n",
    "y_hat = Z_output  # Here, we will use a linear activation, so the output layer input is already the prediction! You could replace this with another activation\n",
    "y_hat.shape  # shape of first forward pass predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the loss and target for this observation. We can see that the random weight initialization is pretty far off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.45032451]] [[207.75432522]]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "## Motivation:\n",
    "With a set of random weights, would a prediction be any good? Probably not. However, the magic of neural networks comes from the method to update the complex system of weights. This is known as back propagation or backprop for short. \n",
    "\n",
    "## Calculating loss\n",
    "The first step in back propagation is to calculate the loss. Recall that *loss* is just another term for error. Several measures facilitate calculating the loss of a regression model. Well-known candidates include mean square error, root-mean square error, mean absolute error, and many others. In general, a loss function measures the degree to which the outputs of your model (i.e., neural network) fit the training data. Higher losses indicate a poorer fit. Thus, after computing the loss, we want to minimize it. This minimization is what network training is about. \n",
    "<br>\n",
    "For illustration, we consider the mean square error (MSE) in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def mse(true, pred):\n",
    "    sse = 0\n",
    "    for i in range(len(true)):\n",
    "        sse += (true[i] - pred[i])**2\n",
    "    mse = (1 / 2*len(true)) * np.sum(sse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's calculate the loss from our first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21883.29273423478"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(y, y_hat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a minimum\n",
    "Like a linear regression, we are working with input values along with layers of weights and a biases (= intercepts in the regression). Our loss function is a function of these three elements. Of these three elements, weights and bias are what we can adjust. So, let’s derive our loss function based on those. We will focus on deriving with respect to weights here, but the process for biases is similar.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\frac{1}{2n} \\sum_{i=1}^n (\\hat y - y)^2\\\\\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{1}{2n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w_j}  (\\hat y - y)^2\\\\\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{1}{2n} \\sum_{i=1}^n 2(\\hat y - y) \\cdot \\frac{\\partial}{\\partial w_j}(\\hat y - y)\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now at this point, we know that the true $y$ is just a scalar. So its derivative is 0. What is the derivative of $\\hat y$ w.r.t. $w_j$? Well, recall the function of $\\hat y$ :\n",
    "\n",
    "$$\\hat y=w_{0}+w_{1}x_{1}+w_{2}x_{2}+...+w_{k}x_{k}$$ \n",
    "\n",
    "So, the derivative w.r.t. to a single $w_j$ will just be the corresponding $x$ value.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\frac{\\partial MSE }{\\partial w_j} = \\frac{2}{2n} \\sum_{i=1}^n (\\hat y - y) \\cdot \\frac{\\partial \\hat y}{\\partial w_j}\\\\\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n (\\hat y - y) \\cdot x \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "note that if $ w_0 $ is the intercept, we would set $ x=1 $.\n",
    "\n",
    "Consider: Our loss function is a function of the true value and our NN’s output. Our NN’s output is a function of the last nodes’ outputs. Our last nodes’ outputs are all a function of their own input values, weights and biases. We’re lucky that we can simply use the chain rule to calculate the derivatives for each parameter! We will then have to organize our derivatives into gradients which are vectors of derivatives of different variables from the same function. For full optimization, we will need to calculate this derivative for every observation as well. We will discuss a simplification technique for this called stochastic gradient descent later.\n",
    "\n",
    "So for each observation, we have to calculate: \n",
    "\n",
    "$$ \\Bigg[ \\frac{\\partial MSE }{\\partial w_0},  \\frac{\\partial MSE }{\\partial w_1}, \\frac{\\partial MSE }{\\partial w_2}, ... ,  \\frac{\\partial MSE }{\\partial w_k} \\Bigg] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_derivative(true, pred, x):\n",
    "    n = len(x)\n",
    "    loss_d = (1/n) * np.sum((pred-true)*x)\n",
    "    return loss_d.reshape(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-35.22604501]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_derivative = mse_derivative(Y[5], y_hat, XX[5])  # loss derivative\n",
    "loss_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_derivative = 1  # activation derivative, if the final activation is f(x) = x, then its derivative is 1\n",
    "activation_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the output layer's output is a function of the following:\n",
    "- the output layer's inputs\n",
    "- the hidden layer's outputs\n",
    "- the hidden layer's inputs\n",
    "- the original X values\n",
    "\n",
    "We will need to update all weights connecting these together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first weights (1, 10)\n",
      "Shape of gradient vector (10, 1)\n",
      "Shape of input (15, 1)\n"
     ]
    }
   ],
   "source": [
    "# Update weights furthest back in the network (between hidden and output layer)\n",
    "gradient_HiddenToOutput = np.dot(loss_derivative*activation_derivative, np.transpose(H))\n",
    "gradient_HiddenToOutput.shape\n",
    "\n",
    "# Update output layer biases\n",
    "gradient_HiddenToOutput_bias = loss_derivative*activation_derivative\n",
    "\n",
    "# Save the error of the output layer\n",
    "pred_errors = loss_derivative * activation_derivative\n",
    "\n",
    "# Find gradient for next step for backpropagation: gradient to update weights between hidden and input layer\n",
    "gradient_InputToHidden = np.dot(W_1.T, pred_errors)\n",
    "print(f'Shape of first weights {W_1.shape}')\n",
    "\n",
    "# Next propagation backwards: derivative of the hidden layer output wrt the hidden layer input (ReLU derivative)\n",
    "gradient_InputToHidden = gradient_InputToHidden * ReLU_derivative(Z_hidden)\n",
    "print(f'Shape of gradient vector {gradient_InputToHidden.shape}')\n",
    "\n",
    "# Derivate of the hidden layer input wrt to the weight matrix connecting the hidden layer to inputs X\n",
    "gradient_InputToHidden = np.dot(gradient_InputToHidden, np.transpose(X))\n",
    "print(f'Shape of input {X.shape}')\n",
    "\n",
    "# Last update: output layer biases\n",
    "gradient_InputToHidden_bias = np.dot(W_1.T, pred_errors) * ReLU_derivative(Z_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving towards a minimum\n",
    "We could update all weights by trying to take a large step down the gradient. However, since there are so many combinations of weights and bias, this just reach one local minimum of many, or it may overshoot the local minimum. It could be that there is a lower local minimum that we cannot yet identify. If we take smaller steps down the gradient, update weights/biases, make a prediction again with these new values and recalculate the gradient, we may find a steeper path which indicates the existence of a lower minimum. This step size is called a learning rate. This slow and iterative way of updating weights is called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few graphics. The one on the left shows the example of trying to fit a curve to points. The figure on the right is a depiction of slowly moving down a error surface to find a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can come back to our scenario and begin the process of updating our weights based on our $learningRate \\cdot gradient$. The formal equation for updating weights and biases looks like this:\n",
    "$$ w_j' = w_j + lr \\cdot \\frac{\\partial MSE }{\\partial w_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "learningRate = 0.001  # define some learning rate\n",
    "\n",
    "# Update weights between hidden and output layer (furthest back)\n",
    "W_1 -= learningRate * gradient_HiddenToOutput\n",
    "# Update bias in output layer\n",
    "B_1 -= learningRate * gradient_HiddenToOutput_bias\n",
    "# Update weights between input and hidden layer (furthest forward)\n",
    "W_0 -= learningRate * gradient_InputToHidden\n",
    "# Update bias in hidden layer\n",
    "B_0 -= learningRate * gradient_InputToHidden_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One solver for gradient descent: SGD\n",
    "As you can imagine, the number of computations for this process can be quite large. It requires one derivative per observation per parameter per step! Stochastic gradient descent (SGD) is one way to greatly simplify this task. This method picks samples randomly and only calculates their derivatives for optimization. By the strictest definition, SGD picks one sample per optimization step, but **it is common to optimize with batches of observations**. This could lead to a more stable path to a local minimum as we average the gradient to the minimum. In the code below, we will select a new variable batch_size, which will randomly select samples to optimize parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping rules\n",
    "What if we find the program is going through too many iterations with little improvement? We need to set a stopping rule to decide when the program should end. There are two very commonly chosen options: specifying a maximum number of epochs (= iterations) allowed or specifying a maximum allowed number of epochs without improvement in the accuracy of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.0001\n",
    "epochs = 250  # stopping rule, how many corrective iterations we will allow\n",
    "batch_size = 250\n",
    "\n",
    "input_dim = XX.shape[1]  # number of variables\n",
    "output_dim = 1  # should be equal to 1 since we are only finding the predicted value for 1 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with average error of 3876.33.\n",
      "Epoch 2 with average error of 3876.22.\n",
      "Epoch 3 with average error of 3876.12.\n",
      "Epoch 4 with average error of 3876.00.\n",
      "Epoch 5 with average error of 3875.88.\n",
      "Epoch 6 with average error of 3875.79.\n",
      "Epoch 7 with average error of 3875.71.\n",
      "Epoch 8 with average error of 3875.60.\n",
      "Epoch 9 with average error of 3875.49.\n",
      "Epoch 10 with average error of 3875.42.\n",
      "Epoch 11 with average error of 3875.37.\n",
      "Epoch 12 with average error of 3875.26.\n",
      "Epoch 13 with average error of 3875.17.\n",
      "Epoch 14 with average error of 3875.10.\n",
      "Epoch 15 with average error of 3875.02.\n",
      "Epoch 16 with average error of 3874.91.\n",
      "Epoch 17 with average error of 3874.83.\n",
      "Epoch 18 with average error of 3874.77.\n",
      "Epoch 19 with average error of 3874.71.\n",
      "Epoch 20 with average error of 3874.66.\n",
      "Epoch 21 with average error of 3874.58.\n",
      "Epoch 22 with average error of 3874.49.\n",
      "Epoch 23 with average error of 3874.47.\n",
      "Epoch 24 with average error of 3874.36.\n",
      "Epoch 25 with average error of 3874.29.\n",
      "Epoch 26 with average error of 3874.22.\n",
      "Epoch 27 with average error of 3874.16.\n",
      "Epoch 28 with average error of 3874.04.\n",
      "Epoch 29 with average error of 3873.95.\n",
      "Epoch 30 with average error of 3873.91.\n",
      "Epoch 31 with average error of 3873.83.\n",
      "Epoch 32 with average error of 3873.74.\n",
      "Epoch 33 with average error of 3873.64.\n",
      "Epoch 34 with average error of 3873.55.\n",
      "Epoch 35 with average error of 3873.39.\n",
      "Epoch 36 with average error of 3873.32.\n",
      "Epoch 37 with average error of 3873.26.\n",
      "Epoch 38 with average error of 3873.15.\n",
      "Epoch 39 with average error of 3873.05.\n",
      "Epoch 40 with average error of 3872.95.\n",
      "Epoch 41 with average error of 3872.88.\n",
      "Epoch 42 with average error of 3872.75.\n",
      "Epoch 43 with average error of 3872.64.\n",
      "Epoch 44 with average error of 3872.56.\n",
      "Epoch 45 with average error of 3872.46.\n",
      "Epoch 46 with average error of 3872.37.\n",
      "Epoch 47 with average error of 3872.29.\n",
      "Epoch 48 with average error of 3872.20.\n",
      "Epoch 49 with average error of 3872.14.\n",
      "Epoch 50 with average error of 3872.06.\n",
      "Epoch 51 with average error of 3871.97.\n",
      "Epoch 52 with average error of 3871.91.\n",
      "Epoch 53 with average error of 3871.81.\n",
      "Epoch 54 with average error of 3871.73.\n",
      "Epoch 55 with average error of 3871.66.\n",
      "Epoch 56 with average error of 3871.61.\n",
      "Epoch 57 with average error of 3871.51.\n",
      "Epoch 58 with average error of 3871.41.\n",
      "Epoch 59 with average error of 3871.30.\n",
      "Epoch 60 with average error of 3871.22.\n",
      "Epoch 61 with average error of 3871.14.\n",
      "Epoch 62 with average error of 3871.02.\n",
      "Epoch 63 with average error of 3870.91.\n",
      "Epoch 64 with average error of 3870.86.\n",
      "Epoch 65 with average error of 3870.78.\n",
      "Epoch 66 with average error of 3870.73.\n",
      "Epoch 67 with average error of 3870.63.\n",
      "Epoch 68 with average error of 3870.57.\n",
      "Epoch 69 with average error of 3870.51.\n",
      "Epoch 70 with average error of 3870.41.\n",
      "Epoch 71 with average error of 3870.28.\n",
      "Epoch 72 with average error of 3870.22.\n",
      "Epoch 73 with average error of 3870.16.\n",
      "Epoch 74 with average error of 3870.10.\n",
      "Epoch 75 with average error of 3870.03.\n",
      "Epoch 76 with average error of 3870.00.\n",
      "Epoch 77 with average error of 3869.94.\n",
      "Epoch 78 with average error of 3869.86.\n",
      "Epoch 79 with average error of 3869.80.\n",
      "Epoch 80 with average error of 3869.72.\n",
      "Epoch 81 with average error of 3869.66.\n",
      "Epoch 82 with average error of 3869.51.\n",
      "Epoch 83 with average error of 3869.42.\n",
      "Epoch 84 with average error of 3869.39.\n",
      "Epoch 85 with average error of 3869.31.\n",
      "Epoch 86 with average error of 3869.23.\n",
      "Epoch 87 with average error of 3869.16.\n",
      "Epoch 88 with average error of 3869.11.\n",
      "Epoch 89 with average error of 3869.06.\n",
      "Epoch 90 with average error of 3868.96.\n",
      "Epoch 91 with average error of 3868.84.\n",
      "Epoch 92 with average error of 3868.82.\n",
      "Epoch 93 with average error of 3868.73.\n",
      "Epoch 94 with average error of 3868.67.\n",
      "Epoch 95 with average error of 3868.57.\n",
      "Epoch 96 with average error of 3868.51.\n",
      "Epoch 97 with average error of 3868.40.\n",
      "Epoch 98 with average error of 3868.31.\n",
      "Epoch 99 with average error of 3868.23.\n",
      "Epoch 100 with average error of 3868.16.\n",
      "Epoch 101 with average error of 3868.08.\n",
      "Epoch 102 with average error of 3867.98.\n",
      "Epoch 103 with average error of 3867.92.\n",
      "Epoch 104 with average error of 3867.85.\n",
      "Epoch 105 with average error of 3867.79.\n",
      "Epoch 106 with average error of 3867.67.\n",
      "Epoch 107 with average error of 3867.54.\n",
      "Epoch 108 with average error of 3867.46.\n",
      "Epoch 109 with average error of 3867.42.\n",
      "Epoch 110 with average error of 3867.33.\n",
      "Epoch 111 with average error of 3867.28.\n",
      "Epoch 112 with average error of 3867.23.\n",
      "Epoch 113 with average error of 3867.15.\n",
      "Epoch 114 with average error of 3867.08.\n",
      "Epoch 115 with average error of 3867.03.\n",
      "Epoch 116 with average error of 3866.92.\n",
      "Epoch 117 with average error of 3866.90.\n",
      "Epoch 118 with average error of 3866.89.\n",
      "Epoch 119 with average error of 3866.83.\n",
      "Epoch 120 with average error of 3866.76.\n",
      "Epoch 121 with average error of 3866.68.\n",
      "Epoch 122 with average error of 3866.64.\n",
      "Epoch 123 with average error of 3866.62.\n",
      "Epoch 124 with average error of 3866.57.\n",
      "Epoch 125 with average error of 3866.50.\n",
      "Epoch 126 with average error of 3866.48.\n",
      "Epoch 127 with average error of 3866.41.\n",
      "Epoch 128 with average error of 3866.32.\n",
      "Epoch 129 with average error of 3866.24.\n",
      "Epoch 130 with average error of 3866.14.\n",
      "Epoch 131 with average error of 3866.08.\n",
      "Epoch 132 with average error of 3866.08.\n",
      "Epoch 133 with average error of 3866.02.\n",
      "Epoch 134 with average error of 3865.98.\n",
      "Epoch 135 with average error of 3865.95.\n",
      "Epoch 136 with average error of 3865.91.\n",
      "Epoch 137 with average error of 3865.84.\n",
      "Epoch 138 with average error of 3865.83.\n",
      "Epoch 139 with average error of 3865.76.\n",
      "Epoch 140 with average error of 3865.70.\n",
      "Epoch 141 with average error of 3865.63.\n",
      "Epoch 142 with average error of 3865.59.\n",
      "Epoch 143 with average error of 3865.55.\n",
      "Epoch 144 with average error of 3865.49.\n",
      "Epoch 145 with average error of 3865.45.\n",
      "Epoch 146 with average error of 3865.43.\n",
      "Epoch 147 with average error of 3865.39.\n",
      "Epoch 148 with average error of 3865.32.\n",
      "Epoch 149 with average error of 3865.27.\n",
      "Epoch 150 with average error of 3865.22.\n",
      "Epoch 151 with average error of 3865.17.\n",
      "Epoch 152 with average error of 3865.12.\n",
      "Epoch 153 with average error of 3865.07.\n",
      "Epoch 154 with average error of 3865.06.\n",
      "Epoch 155 with average error of 3865.01.\n",
      "Epoch 156 with average error of 3864.93.\n",
      "Epoch 157 with average error of 3864.93.\n",
      "Epoch 158 with average error of 3864.92.\n",
      "Epoch 159 with average error of 3864.87.\n",
      "Epoch 160 with average error of 3864.81.\n",
      "Epoch 161 with average error of 3864.75.\n",
      "Epoch 162 with average error of 3864.69.\n",
      "Epoch 163 with average error of 3864.63.\n",
      "Epoch 164 with average error of 3864.64.\n",
      "Epoch 165 with average error of 3864.61.\n",
      "Epoch 166 with average error of 3864.54.\n",
      "Epoch 167 with average error of 3864.51.\n",
      "Epoch 168 with average error of 3864.43.\n",
      "Epoch 169 with average error of 3864.39.\n",
      "Epoch 170 with average error of 3864.42.\n",
      "Epoch 171 with average error of 3864.41.\n",
      "Epoch 172 with average error of 3864.33.\n",
      "Epoch 173 with average error of 3864.24.\n",
      "Epoch 174 with average error of 3864.15.\n",
      "Epoch 175 with average error of 3864.06.\n",
      "Epoch 176 with average error of 3863.99.\n",
      "Epoch 177 with average error of 3863.96.\n",
      "Epoch 178 with average error of 3863.97.\n",
      "Epoch 179 with average error of 3863.89.\n",
      "Epoch 180 with average error of 3863.85.\n",
      "Epoch 181 with average error of 3863.78.\n",
      "Epoch 182 with average error of 3863.74.\n",
      "Epoch 183 with average error of 3863.73.\n",
      "Epoch 184 with average error of 3863.74.\n",
      "Epoch 185 with average error of 3863.63.\n",
      "Epoch 186 with average error of 3863.59.\n",
      "Epoch 187 with average error of 3863.58.\n",
      "Epoch 188 with average error of 3863.51.\n",
      "Epoch 189 with average error of 3863.47.\n",
      "Epoch 190 with average error of 3863.43.\n",
      "Epoch 191 with average error of 3863.48.\n",
      "Epoch 192 with average error of 3863.41.\n",
      "Epoch 193 with average error of 3863.40.\n",
      "Epoch 194 with average error of 3863.43.\n",
      "Epoch 195 with average error of 3863.48.\n",
      "Epoch 196 with average error of 3863.42.\n",
      "Epoch 197 with average error of 3863.36.\n",
      "Epoch 198 with average error of 3863.45.\n",
      "Epoch 199 with average error of 3863.41.\n",
      "Epoch 200 with average error of 3863.34.\n",
      "Epoch 201 with average error of 3863.24.\n",
      "Epoch 202 with average error of 3863.23.\n",
      "Epoch 203 with average error of 3863.33.\n",
      "Epoch 204 with average error of 3863.36.\n",
      "Epoch 205 with average error of 3863.49.\n",
      "Epoch 206 with average error of 3863.50.\n",
      "Epoch 207 with average error of 3863.46.\n",
      "Epoch 208 with average error of 3863.40.\n",
      "Epoch 209 with average error of 3863.35.\n",
      "Epoch 210 with average error of 3863.38.\n",
      "Epoch 211 with average error of 3863.45.\n",
      "Epoch 212 with average error of 3863.35.\n",
      "Epoch 213 with average error of 3863.42.\n",
      "Epoch 214 with average error of 3863.32.\n",
      "Epoch 215 with average error of 3863.34.\n",
      "Epoch 216 with average error of 3863.25.\n",
      "Epoch 217 with average error of 3863.25.\n",
      "Epoch 218 with average error of 3863.29.\n",
      "Epoch 219 with average error of 3863.38.\n",
      "Epoch 220 with average error of 3863.40.\n",
      "Epoch 221 with average error of 3863.35.\n",
      "Epoch 222 with average error of 3863.37.\n",
      "Epoch 223 with average error of 3863.29.\n",
      "Epoch 224 with average error of 3863.26.\n",
      "Epoch 225 with average error of 3863.21.\n",
      "Epoch 226 with average error of 3863.21.\n",
      "Epoch 227 with average error of 3863.20.\n",
      "Epoch 228 with average error of 3863.22.\n",
      "Epoch 229 with average error of 3863.29.\n",
      "Epoch 230 with average error of 3863.30.\n",
      "Epoch 231 with average error of 3863.23.\n",
      "Epoch 232 with average error of 3863.30.\n",
      "Epoch 233 with average error of 3863.35.\n",
      "Epoch 234 with average error of 3863.34.\n",
      "Epoch 235 with average error of 3863.36.\n",
      "Epoch 236 with average error of 3863.44.\n",
      "Epoch 237 with average error of 3863.40.\n",
      "Epoch 238 with average error of 3863.46.\n",
      "Epoch 239 with average error of 3863.35.\n",
      "Epoch 240 with average error of 3863.40.\n",
      "Epoch 241 with average error of 3863.38.\n",
      "Epoch 242 with average error of 3863.47.\n",
      "Epoch 243 with average error of 3863.55.\n",
      "Epoch 244 with average error of 3863.54.\n",
      "Epoch 245 with average error of 3863.65.\n",
      "Epoch 246 with average error of 3863.66.\n",
      "Epoch 247 with average error of 3863.86.\n",
      "Epoch 248 with average error of 3863.95.\n",
      "Epoch 249 with average error of 3863.91.\n",
      "Epoch 250 with average error of 3863.90.\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "while iteration < epochs:\n",
    "\n",
    "    # Process one batch of random observations per epoch/iteration\n",
    "    random_batch = random.sample(range(0, XX.shape[0]), batch_size)  # choose observations for random batch\n",
    "    \n",
    "    HiddenToOutput_gradient_list = [] # to store the gradients based on each observation in batch\n",
    "    InputToHidden_gradient_list = []\n",
    "\n",
    "    # Update weights and biases one at a time with each random observation's error (all steps before, just in a loop together)\n",
    "    for obs in random_batch:\n",
    "\n",
    "        # Select feature values and target value for random observation\n",
    "        X = np.array(XX[obs]).reshape((input_dim, 1))\n",
    "        y = Y[obs].reshape((1, 1))\n",
    "\n",
    "        # Compute the forward pass through the network all the way up to the final output\n",
    "        Z_hidden = np.dot(W_0, X) + B_0\n",
    "        H = ReLU(Z_hidden)\n",
    "        Z_output = np.dot(W_1, H) + B_1\n",
    "        y_hat = Z_output  # prediction from first iteration\n",
    "\n",
    "        # Gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = mse_derivative(Y[obs], y_hat, XX[obs]) * 1 # linear activation derivative is 1\n",
    "        pred_errors = gradient_HiddenToOutput\n",
    "\n",
    "        # Gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(W_1.T, pred_errors) * ReLU_derivative(Z_hidden)\n",
    "        \n",
    "        HiddenToOutput_gradient_list.append(gradient_HiddenToOutput)\n",
    "        InputToHidden_gradient_list.append(gradient_InputToHidden)\n",
    "\n",
    "    mean_gradient_HiddenToOutput = np.mean(HiddenToOutput_gradient_list, axis=0) # average optimal movemnt over batch\n",
    "    mean_gradient_InputToHidden = np.mean(InputToHidden_gradient_list, axis=0)\n",
    "    \n",
    "    # Update biases according to learning rate and gradient\n",
    "    B_1 -= learningRate * mean_gradient_HiddenToOutput\n",
    "    B_0 -= learningRate * mean_gradient_InputToHidden\n",
    "\n",
    "    # Update weights according to learning rate and gradient\n",
    "    W_1 -= learningRate * np.dot(mean_gradient_HiddenToOutput, np.transpose(Z_hidden))\n",
    "    W_0 -= learningRate * np.dot(mean_gradient_InputToHidden, np.transpose(X))\n",
    "\n",
    "    # Check how well the model does on all observations by passing them through a forward pass with most up to date weights\n",
    "    XX_reshaped = np.array(XX).reshape((inputLayer_size, n))  # all observations\n",
    "    Y_reshaped = Y.reshape((outputLayer_size, n))\n",
    "    Z_hidden = np.dot(W_0, XX_reshaped) + B_0  # first hidden layer inputs\n",
    "    H = ReLU(Z_hidden)  # hidden layer output (after activation)\n",
    "    Z_output = np.dot(W_1, H) + B_1  # input to output layer\n",
    "    Y_hat = Z_output\n",
    "    \n",
    "    # Calculate loss for entire model\n",
    "    iteration_loss_square = mse(Y_reshaped, Y_hat)\n",
    "    iteration_loss = math.sqrt(iteration_loss_square)\n",
    "    loss_log.append(iteration_loss)\n",
    "\n",
    "    # Development of the loss as average over observation-level losses\n",
    "    print(f'Epoch {iteration+1} with average error of {iteration_loss:.2f}.')\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "plt.plot(loss_log)\n",
    "plt.title(\"Loss over Iterations of NN\")\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! As the epochs continue, we can see iterations show improvement in errors for the network. Important questions:\n",
    "- What could prevent the error from continuously falling?\n",
    "- What are some other good ideas for stopping rules for our algorithm? Or, how can we manipulate the learning rate?\n",
    "- Instead of looking at average loss per batch, could you think of a better way to measure model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of back propagation\n",
    "So, after setting up the neural network, we need to:\n",
    "- Calculate loss from the forward pass\n",
    "- Calculate the gradient of random observations (SGD)\n",
    "- Update weights by moving down the gradient with a step size specified by your learning rate\n",
    "- Make new predictions with the new weights\n",
    "- Repeat the these steps until you reach a stopping rule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
