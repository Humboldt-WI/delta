{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #4: Neural Networking (NN) Primer\n",
    "## Part 3 of 3\n",
    "\n",
    "This is the third of three notebooks which will cover our initial steps into creating our own neural network. The parts are as follows:\n",
    "\n",
    "This is the final of three notebooks which will cover the foundations of neural networks. Please review the first two notebooks if you haven't yet, as this notebook assumes you understand the first two. In the first notebook, we discussed how the algorithm can be initialized randomly. The second notebook focused on training the network through back propagation. Training is the process in which we update the weights to maximize the fit of our network to (the training) data. Network training is, therefore, equivalent to maximum likelihood estimation in regression analysis. \n",
    "\n",
    "This notebook will be exploring a different problem that neural networks are well-suited to fix: classification. We will be rewiring the same code form part 2, only this time, our network will classify samples. Differences will be explained as we go.\n",
    "\n",
    "- Neural Network Structures and the Forward Pass\n",
    "- Coding a Back Propagation\n",
    "- Changes for Classification\n",
    "\n",
    "## This notebook's topic: Adapting what we know for a classification network ##\n",
    " 1. Reviewing classification problems\n",
    " 2. Highlighting differences in the algorithm\n",
    " 3. Revising our code to solve a classification problem\n",
    " 4. A cheat sheet for the most basic neural network problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "range_for_demo = np.linspace(-5, 5, 100)\n",
    "\n",
    "import random\n",
    "random.seed(888)  # set seed for reproducibility\n",
    "np.random.seed(888)\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of classification problems\n",
    "\n",
    "There are a few types of problems which require us to label the classes of data. Here is a quick summary:\n",
    "\n",
    "**Binary Classification** tries to predict which of two options is most likely. Eg. Will this customer spend money today (yes/no)? Will rain fall today (yes/no)?\n",
    " - Single dummy: binary target coded as a single binary variable representing the probability of an event occurring or label 1 (eg. one variable for purchase, no purchase is inferred as the inverse)\n",
    " - Two-dummy: one dummy is the probability of the non-event/label 0, the second being the probability of the event/label 1 (eg. one variable for no purchase and one variable for purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single dummy approach\n",
    "Y = [1, 0, 1]\n",
    "\n",
    "# two-dummy approach (same information as the single dummy above)\n",
    "Y_0 = [0, 1, 0]\n",
    "Y_1 = [1, 0, 1]  # note that this vector is identical to single dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Label Multi-Class Problems** tries to predict which of 3 or more options are most likely but only ONE class is possible. Eg. Is this image a cat, dog or bird? It is recommended to code this as a dummy variable.\n",
    "\n",
    "**Multiple Label Multi-Class Problems** tries to predict which of 3 or more options are most likely and when multiple classes are possible. Eg. Which of these products will be purchased? It is recommended to code this as a dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in NNs for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data Generation: Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "k = 2\n",
    "\n",
    "XX, Y = make_blobs(n_samples=n, n_features=k, centers=2, random_state=888, cluster_std=1.5)\n",
    "\n",
    "Y = Y.reshape(n,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "For the most part, the network architecture required for binary classification networks is very similar to those required for regression networks. However, we do have to be careful about specifying the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer size\n",
    "For regression problems, we had a constant output layer size of 1 neuron because we were only predicting one number.\n",
    "\n",
    "For **binary classification** problems where the target is coded as a single dummy variable, you will need 1 node in your output layer. If there are two dummy variables for the target (one dummy for class 0 and one dummy for class 1), you will need 2 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture:\n",
    "inputLayer_size = 2      # number of features in X, in this case we only have 2\n",
    "hiddenLayer_size = 10    # number of hidden layer nodes\n",
    "outputLayer_size = 1     # number of values to predict, in this case, our target is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, for the initialization, we will only take the 5th observation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will not work since, for matrix multiplication, we need a kx1 vector\n",
    "X = np.array(XX[5]).reshape((inputLayer_size, 1))\n",
    "X.shape  # This should be kx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Y[5].reshape((outputLayer_size, 1))\n",
    "y.shape  # since our data is a single value regression for a single observation, we should see (1, 1) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization\n",
    "For the most part, weight initialization is pretty much identical for classification problems. Just be careful of your output layer size if you have more than one target column (multiple classes or two-dummy encoded binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = np.sqrt(6 / (inputLayer_size + outputLayer_size))  # Recommended weight initialization\n",
    "\n",
    "# Random weight and bias initialization\n",
    "W_0 = np.random.uniform(-limit, limit, (hiddenLayer_size, inputLayer_size))    # input to hidden layer weights\n",
    "W_1 = np.random.uniform(-limit, limit, (outputLayer_size, hiddenLayer_size))  # hidden to output layer weights\n",
    "\n",
    "B_0 = np.ones((hiddenLayer_size,1))  # input to hiden layer biases\n",
    "B_1 = np.ones((outputLayer_size,1))  # hidden to output layer biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.39293216],\n",
       "       [ 12.66778464],\n",
       "       [-10.72624367],\n",
       "       [  2.7568921 ],\n",
       "       [  4.14130474]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_hidden = np.dot(W_0, X) + B_0\n",
    "Z_hidden[:5]  # these are the first five nodes' input values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Remember that you should distinguish your activation functions in two groups:\n",
    "- Hidden layer activations: here you can experiment with any previously explained activation function\n",
    "- Final layer activation: this one needs to suit your output. We mentioned using a ReLU for regression. Below we will discuss classification activations.\n",
    "\n",
    "For the hidden layer in our current network, let's take $tanh$ this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function that we will use in this scenario and its derivative\n",
    "def tanH(x):\n",
    "    return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "\n",
    "def tanH_derivative(x):\n",
    "    return 1-tanH(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [ 1.        ],\n",
       "       [-1.        ],\n",
       "       [ 0.99197071],\n",
       "       [ 0.99949437]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = tanH(Z_hidden)\n",
    "H[:5]  # these are the first five nodes' output values (after activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final activation function\n",
    "\n",
    "We talked about a few possible activation functions such as ReLU, leaky ReLU, tanh and sigmoid. We can experiment with any of these if they are not in the final layer. However, we need to pay attention to the final layer's activation because it determines the range of values that will be output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid and binary classification\n",
    "**If your target is a single dummy**, you will need your network to output a value between 0 and 1 as the probability of class 1. The best final activation function for the output layer will be **sigmoid** whose outputs are always squeezed into a probability between 0 and 1. tanh can also be used but note that it ranges between -1 and 1, so you will need to rescale it. If you would like to see a more detailed breakdown of the sigmoid function, you can find it in the section on activation functions in part 2 of this series of tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1-s)\n",
    "\n",
    "plt.plot(range_for_demo, sigmoid(range_for_demo), label='f(x)')\n",
    "plt.plot(range_for_demo, sigmoid_derivative(range_for_demo), label=\"f(x)'\")\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax and binary classification\n",
    "\n",
    "**If your target is two dummy variables**, you'll have 2 output neurons. The first neuron outputs the probability of class 0 and the second outputs the probability of class 1. We want the sum of the probability of both classes to be 1. In this case, we have a final activation function called **softmax** which will make sure the output squeeze all probabilities sum to 1.\n",
    "\n",
    "$$ \\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_{j=1} e^{x_j} } $$\n",
    "\n",
    "#### What about more classes?\n",
    "This question is answerd fairly simply by determining whether only one or multiple classes are possible per observation.\n",
    "- Only **one class possible** (eg. classifying images of dogs, cats and birds): you'd want all probabilities to sum to 1, use **softmax**\n",
    "- **Multiple classes possible** (eg. what will be in this customer's basket?): probabilities don't have to sum to 1, use **sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the softmax output for derivation\n",
    "\n",
    "This function takes a $X$ as a vector and produces a vector of probabilities which sum to 1. Since we are predicting for classes, this vector will have the dimension of n_class x 1.\n",
    "\n",
    "So, in essence, $\\text{softmax}(x)$ does the following:\n",
    "$$ \\text{softmax} \\bigg( \\begin{bmatrix}  x_{1} \\\\  x_{2}\\\\   \\vdots\\\\ x_{n}\\\\ \\end {bmatrix} \\bigg) \\rightarrow\n",
    "\\begin{bmatrix}  s_{1} \\\\  s_{2} \\\\  \\vdots \\\\ s_{n} \\end {bmatrix} $$\n",
    "\n",
    "If we want to derive this function, we will have to derive each output w.r.t. $x_1$ through $x_n$. This will create a Jacobian (square) matrix:\n",
    "$$ \\begin{bmatrix} \\frac{\\partial s_1}{\\partial x_1} & \\frac{\\partial s_1}{\\partial x_2} & \\ldots \\ & \\frac{\\partial s_1}{\\partial x_n} \\\\ \\frac{\\partial s_2}{\\partial x_1} & \\frac{\\partial s_2}{\\partial x_2} \\\\  \\vdots & \\vdots \\\\  \\frac{\\partial s_1}{\\partial x_n} & \\frac{\\partial s_n}{\\partial x_2} & \\ldots\\ & \\frac{\\partial s_n}{\\partial x_n} \\\\ \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the softmax\n",
    "\n",
    "If we want to derive this function, we would have to use the quotient rule as we have used in many other calculations before.\n",
    "\n",
    "$$\\begin{aligned} &f(x) = \\frac{g(x)}{h(x)} \\\\\n",
    "& f'(x) = \\frac{g'(x)h(x) - g(x)h'(x)}{[h(x)]^2} \\end{aligned}$$\n",
    "\n",
    "Here, $g(x)=e^{x_i}$ and $h(x)=\\sum_{j=1} e^{x_j}$. There are two situations which we will have when we derive with respect to $x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: $i=j$ (Jacobian matrix diagonal)\n",
    "$$g(x)=e^{x_j}$$\n",
    "$$ \\frac{\\partial g(x)}{\\partial x_j}=e^{x_j}$$\n",
    "\n",
    "$$h(x)=\\sum_{j=1} e^{x_j}$$\n",
    "$$ \\frac{\\partial h(x)}{\\partial x_j}=e^{x_j}$$\n",
    "\n",
    "So, if we want to find the partial derivatives of the softmax function, we would need to do the following for this case:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = \\frac{\\partial \\text{softmax}}{\\partial x_j} \\\\\n",
    "& = \\frac{e^{x_j} \\cdot \\sum_{k=1} e^{x_k} - e^{x_j} \\cdot e^{x_j}}{\\big[ \\sum_{k=1} e^{x_k} \\big]^2 } \\\\\n",
    "& = \\frac{e^{x_j} \\big( \\sum_{k=1} e^{x_k} - e^{x_j} \\big) } {\\big[ \\sum_{k=1} e^{x_k} \\big]^2 } \\\\\n",
    "& = \\frac{e^{x_j}} { \\sum_{k=1} e^{x_k} } \\cdot \\frac{\\sum_{k=1} e^{x_k} - e^{x_j}}{ \\sum_{k=1} e^{x_k} } \\\\\n",
    "& = \\text{softmax} (1- \\text{softmax})\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: $i \\neq j$ (off-diagonal elements)\n",
    "$$g(x)=e^{x_i}$$\n",
    "$$ \\frac{\\partial g(x)}{\\partial x_j}=0$$\n",
    "\n",
    "$$h(x)=\\sum_{j=1} e^{x_j}$$\n",
    "$$ \\frac{\\partial h(x)}{\\partial x_j}=e^{x_j}$$\n",
    "\n",
    "\n",
    "And for this case, the derivative is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = \\frac{\\partial \\text{softmax}}{\\partial x_j} \\\\\n",
    "& = \\frac{0 \\cdot \\sum_{k=1} e^{x_k} - e^{x_j} \\cdot e^{x_i}}{\\big[ \\sum_{k=1} e^{x_k} \\big]^2 } \\\\\n",
    "& = - \\frac{e^{x_j}}{\\sum_{k=1} e^{x_k}} \\cdot \\frac{e^{x_i}}{\\sum_{k=1} e^{x_k}} \\\\\n",
    "& = - \\text{softmax} \\cdot \\text{softmax}\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we will only plot the function for simplicity. The derivate is more difficult to plot (see, e.g., lecture slides)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "plt.plot(range_for_demo, softmax(range_for_demo), label=\"f(x)\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.59748306]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_output = np.dot(W_1, H) + B_1  # sum of next layer of weights*respective node then add bias\n",
    "Z_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99002337]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = sigmoid(Z_output)  # Apply activation for output layer's output\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Classification\n",
    "\n",
    "### Cross-Entropy\n",
    "Entropy $H(y)$ is a term from Information Theory which had great impact on the field of communication. It signifies the optimal number of bits to encode a certain information content. $y_c$ is the probability of the nth event, or in our case, class:\n",
    "\n",
    "$$H(y) = \\sum_c y_c \\cdot \\log \\frac{1}{y_c} = -\\sum_c y_c \\cdot \\log y_c$$\n",
    "\n",
    "Now the cross-entropy $H(y,\\hat{y})$ is the number of bits we'll need if we encode symbols from $y$ using the wrong tool $\\hat{y}$. Cross-entropy is always bigger or equal to entropy. Mind that $c$ stands for the number of classes. \n",
    "\n",
    "$$H(y, \\hat{y}) = \\sum_c y_c \\cdot \\log \\frac{1}{\\hat{y}_c} = -\\sum_c y_c \\cdot \\log \\hat{y}_c$$\n",
    "\n",
    "Interestingly enough, the The KL divergence, which you might know from other courses is simply the difference between cross-entropy and entropy:\n",
    "$$\\mbox{KL}(y~||~\\hat{y})\n",
    "= \\sum_c y_c \\log \\frac{1}{\\hat{y}_c} - \\sum_c y_c \\log \\frac{1}{y_c}\n",
    "= \\sum_c y_c \\log \\frac{y_c}{\\hat{y}_c}$$\n",
    "\n",
    "We would be calculating the cross-entropy for every pair of true/estimated probabilities and averaging it over the sample or batch - this will be our loss function *L* that we will ultimately want to minimise (class i, sample j):\n",
    "\n",
    "$$L=-\\frac{1}{n}\\sum_i \\sum_c y_{i,c} \\log(\\hat{y}_{i,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross-entropy\n",
    "For **binary classification**, we would use the special case of the equation above with 2 classes called binary cross-entropy (note there are other options like hinge loss).\n",
    "\n",
    "$$L=-\\frac{1}{n}\\sum_i \\big( y_{i,1}  \\log(\\hat{y}_{i,1}) + y_{i,2} \\log(\\hat{y}_{i,2}) \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the loss function\n",
    "Now, we need to minimize the loss function. Let's derive the binary cross entropy function for a single observation. We will remove the $\\frac{1}{n}$ since it can be divided out in the minimization:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = \\frac{\\partial L_i}{\\partial w} \\\\\n",
    "& = - \\frac{\\partial}{\\partial w} y_{1}  \\log(\\hat{y}_{1}) + \\frac{\\partial}{\\partial w}  y_{2} \\log(\\hat{y}_{2}) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "At this point it is important to remember that the true value is just a constant. The estimate is a function determined by the sigmoid. So, we treat $y$ as a constant, and use the chain rule on $log(\\hat y_i)$. We also can replace $y_2$ with $1-y$ and $y_1$ with $y$ since we only have two probabilities/possibilities which are complements of each other. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = - \\frac{y_{1}}{\\hat{y}_{1}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y}_{1} - \\frac{y_{2}}{\\hat{y}_{2}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y}_{2} \\\\\n",
    "& = - \\frac{y}{\\hat{y}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y} - \\frac{1-{y}}{1-\\hat{y}} \\cdot \\frac{\\partial}{\\partial w} (1-\\hat{y}) \\\\\n",
    "& = - \\frac{y}{\\hat{y}} \\cdot \\frac{\\partial}{\\partial w}\\hat{y} + \\frac{1-{y}}{1-\\hat{y}} \\cdot \\frac{\\partial}{\\partial w} \\hat{y} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "We know that the final equation which determines $\\hat y$ is the final sigmoid activation. So, we can replace $\\hat y$ with the sigmoid and derive it where necessary (full derivation in part 1 of these tutorials).\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& = - \\frac{y}{\\text{sigmoid}} \\cdot \\frac{\\partial}{\\partial w} \\text{sigmoid} + \\frac{1-{y}}{1-\\text{sigmoid}} \\cdot \\frac{\\partial}{\\partial w} \\text{sigmoid}  \\\\\n",
    "& = - \\frac{y}{\\text{sigmoid}} \\cdot \\text{sigmoid}\\cdot (1-\\text{sigmoid}) + \\frac{1-{y}}{1-\\text{sigmoid}} \\cdot (\\text{sigmoid}\\cdot (1-\\text{sigmoid}))  \\\\\n",
    "& = - y \\cdot (1-\\text{sigmoid}) + (1-y) \\cdot \\text{sigmoid} \\\\\n",
    "& = - y + y \\cdot \\text{sigmoid} + \\text{sigmoid}-y \\cdot \\text{sigmoid} \\\\\n",
    "& = \\text{sigmoid}-{y} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The sigmoid at the end is also the prediction, so that means: $$ \\text{sigmoid}-{y} = \\hat{y}-{y} $$ The extreme simplicity of this function's derivative is what makes it so ideal for a loss function, especially when the number of calculations starts to grow in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary cros entropy loss function\n",
    "def bce(true, pred):\n",
    "    eps = 1e-50 # small epsilon so that the log never tries to div by 0\n",
    "    return - np.mean(\n",
    "        np.multiply(true, np.log(pred + eps)) + np.multiply((1-true), np.log(1 - pred + eps)))\n",
    "\n",
    "def bce_derivative(pred, true):\n",
    "    return pred - true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the intial loss for observation 5, the derivative of the loss function and final activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01002673136404279"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = bce(Y[5], y_hat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00997663]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_derivative = bce_derivative(y_hat, Y[5])\n",
    "loss_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0098771]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_activation_derivative = sigmoid_derivative(Z_output)\n",
    "final_activation_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization based on loss\n",
    "Note that the steps to calculate loss below have now been replaced with the derivative of binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first weights (1, 10)\n",
      "Shape of gradient vector (10, 1)\n",
      "Shape of input (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Update weights furthest back in the network (between hidden and output layer)\n",
    "gradient_HiddenToOutput = np.dot(loss_derivative*final_activation_derivative, np.transpose(H))\n",
    "gradient_HiddenToOutput.shape\n",
    "\n",
    "# Update output layer biases\n",
    "gradient_HiddenToOutput_bias = loss_derivative*final_activation_derivative\n",
    "\n",
    "# Save the error of the output layer\n",
    "pred_errors = loss_derivative * final_activation_derivative\n",
    "\n",
    "# Find gradient for next step for backpropagation: gradient to update weights between hidden and input layer\n",
    "gradient_InputToHidden = np.dot(W_1.T, pred_errors)\n",
    "print(f'Shape of first weights {W_1.shape}')\n",
    "\n",
    "# Next propagation backwards: derivative of the hidden layer output wrt the hidden layer input (tanh derivative)\n",
    "gradient_InputToHidden = gradient_InputToHidden * tanH_derivative(Z_hidden)\n",
    "print(f'Shape of gradient vector {gradient_InputToHidden.shape}')\n",
    "\n",
    "# Derivate of the hidden layer input wrt to the weight matrix connecting the hidden layer to inputs X\n",
    "gradient_InputToHidden = np.dot(gradient_InputToHidden, np.transpose(X))\n",
    "print(f'Shape of input {X.shape}')\n",
    "\n",
    "# Last update: output layer biases\n",
    "gradient_InputToHidden_bias = np.dot(W_1.T, pred_errors) * tanH_derivative(Z_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial weight correction\n",
    "There are no big differences for the rest of the neural network, we are just going to replace the elements that were specifically for a regression network before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "learningRate = 0.0001  # define some learning rate\n",
    "\n",
    "# Update weights between hidden and output layer (furthest back)\n",
    "W_1 -= learningRate * gradient_HiddenToOutput\n",
    "# Update bias in output layer\n",
    "B_1 -= learningRate * gradient_HiddenToOutput_bias\n",
    "# Update weights between input and hidden layer (furthest forward)\n",
    "W_0 -= learningRate * gradient_InputToHidden\n",
    "# Update bias in hidden layer\n",
    "B_0 -= learningRate * gradient_InputToHidden_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting learning rate, epochs and batch sizes\n",
    "As before, we will need to set a few more parameters which parameters which we can later adjust for better optimization of the network:\n",
    "- Learning rate: how much the algorithm will move in the negative direction of the gradient, too large and we will likely overshoot a local minimum, too low and no improvement will be seen\n",
    "- Epochs: how many rounds to train the model, too few could be too little for improvement, too many could cause the capture of too much noise in the data\n",
    "- Batch size: the amount of observations that we will be randomly selecting for the optimization of our network, especially important when there are many nodes, features and individual observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.0001\n",
    "epochs =  85  # stopping rule, how many corrective iterations we will allow\n",
    "batch_size = 250\n",
    "\n",
    "input_dim = XX.shape[1]  # number of variables\n",
    "output_dim = 1  # should be equal to 1 since we are only finding the predicted value for 1 regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewire SGD for binary classification\n",
    "In the same code that we used before, we just have to replace the final activation with a sigmoid function and the loss function to binary cross-entropy. The derivative for these two activations will also be necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with average error of 1.48.\n",
      "Epoch 2 with average error of 1.48.\n",
      "Epoch 3 with average error of 1.48.\n",
      "Epoch 4 with average error of 1.47.\n",
      "Epoch 5 with average error of 1.47.\n",
      "Epoch 6 with average error of 1.46.\n",
      "Epoch 7 with average error of 1.46.\n",
      "Epoch 8 with average error of 1.46.\n",
      "Epoch 9 with average error of 1.45.\n",
      "Epoch 10 with average error of 1.45.\n",
      "Epoch 11 with average error of 1.44.\n",
      "Epoch 12 with average error of 1.44.\n",
      "Epoch 13 with average error of 1.43.\n",
      "Epoch 14 with average error of 1.43.\n",
      "Epoch 15 with average error of 1.43.\n",
      "Epoch 16 with average error of 1.42.\n",
      "Epoch 17 with average error of 1.42.\n",
      "Epoch 18 with average error of 1.41.\n",
      "Epoch 19 with average error of 1.41.\n",
      "Epoch 20 with average error of 1.40.\n",
      "Epoch 21 with average error of 1.40.\n",
      "Epoch 22 with average error of 1.39.\n",
      "Epoch 23 with average error of 1.39.\n",
      "Epoch 24 with average error of 1.38.\n",
      "Epoch 25 with average error of 1.38.\n",
      "Epoch 26 with average error of 1.37.\n",
      "Epoch 27 with average error of 1.37.\n",
      "Epoch 28 with average error of 1.36.\n",
      "Epoch 29 with average error of 1.36.\n",
      "Epoch 30 with average error of 1.35.\n",
      "Epoch 31 with average error of 1.34.\n",
      "Epoch 32 with average error of 1.34.\n",
      "Epoch 33 with average error of 1.33.\n",
      "Epoch 34 with average error of 1.32.\n",
      "Epoch 35 with average error of 1.32.\n",
      "Epoch 36 with average error of 1.31.\n",
      "Epoch 37 with average error of 1.31.\n",
      "Epoch 38 with average error of 1.30.\n",
      "Epoch 39 with average error of 1.29.\n",
      "Epoch 40 with average error of 1.29.\n",
      "Epoch 41 with average error of 1.28.\n",
      "Epoch 42 with average error of 1.27.\n",
      "Epoch 43 with average error of 1.27.\n",
      "Epoch 44 with average error of 1.26.\n",
      "Epoch 45 with average error of 1.25.\n",
      "Epoch 46 with average error of 1.24.\n",
      "Epoch 47 with average error of 1.24.\n",
      "Epoch 48 with average error of 1.23.\n",
      "Epoch 49 with average error of 1.22.\n",
      "Epoch 50 with average error of 1.21.\n",
      "Epoch 51 with average error of 1.21.\n",
      "Epoch 52 with average error of 1.20.\n",
      "Epoch 53 with average error of 1.19.\n",
      "Epoch 54 with average error of 1.18.\n",
      "Epoch 55 with average error of 1.17.\n",
      "Epoch 56 with average error of 1.16.\n",
      "Epoch 57 with average error of 1.15.\n",
      "Epoch 58 with average error of 1.14.\n",
      "Epoch 59 with average error of 1.13.\n",
      "Epoch 60 with average error of 1.12.\n",
      "Epoch 61 with average error of 1.11.\n",
      "Epoch 62 with average error of 1.10.\n",
      "Epoch 63 with average error of 1.09.\n",
      "Epoch 64 with average error of 1.08.\n",
      "Epoch 65 with average error of 1.07.\n",
      "Epoch 66 with average error of 1.05.\n",
      "Epoch 67 with average error of 1.04.\n",
      "Epoch 68 with average error of 1.03.\n",
      "Epoch 69 with average error of 1.02.\n",
      "Epoch 70 with average error of 1.01.\n",
      "Epoch 71 with average error of 1.00.\n",
      "Epoch 72 with average error of 0.98.\n",
      "Epoch 73 with average error of 0.97.\n",
      "Epoch 74 with average error of 0.96.\n",
      "Epoch 75 with average error of 0.95.\n",
      "Epoch 76 with average error of 0.93.\n",
      "Epoch 77 with average error of 0.92.\n",
      "Epoch 78 with average error of 0.91.\n",
      "Epoch 79 with average error of 0.90.\n",
      "Epoch 80 with average error of 0.89.\n",
      "Epoch 81 with average error of 0.89.\n",
      "Epoch 82 with average error of 0.88.\n",
      "Epoch 83 with average error of 0.87.\n",
      "Epoch 84 with average error of 0.87.\n",
      "Epoch 85 with average error of 0.87.\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "while iteration < epochs:\n",
    "\n",
    "    # Process one batch of random observations per epoch/iteration\n",
    "    random_batch = random.sample(range(0, XX.shape[0]), batch_size)  # choose observations for random batch\n",
    "\n",
    "    # Update weights and biases one at a time with each random observation's error (all steps before, just in a loop together)\n",
    "    for obs in random_batch:\n",
    "\n",
    "        # Select feature values and target value for random observation\n",
    "        X = np.array(XX[obs]).reshape((input_dim, 1))\n",
    "        y = Y[obs].reshape((1, 1))\n",
    "\n",
    "        # Compute the forward pass through the network all the way up to the final output\n",
    "        Z_hidden = np.dot(W_0, X) + B_0\n",
    "        H = tanH(Z_hidden)\n",
    "        Z_output = np.dot(W_1, H) + B_1\n",
    "        y_hat = sigmoid(Z_output)  # final activation is sigmoid instead of ReLU\n",
    "\n",
    "        # Gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = bce_derivative(Y[obs], y_hat) * sigmoid_derivative(y_hat) # use derivative of BCE loss and sigmoid\n",
    "        pred_errors = gradient_HiddenToOutput\n",
    "\n",
    "        # Gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(W_1.T, pred_errors) * tanH_derivative(Z_hidden)\n",
    "\n",
    "        # Update biases according to learning rate and gradient\n",
    "        B_1 -= learningRate * gradient_HiddenToOutput\n",
    "        B_0 -= learningRate * gradient_InputToHidden\n",
    "\n",
    "        # Update weights according to learning rate and gradient\n",
    "        W_1 -= learningRate * np.dot(gradient_HiddenToOutput, np.transpose(Z_hidden))\n",
    "        W_0 -= learningRate * np.dot(gradient_InputToHidden, np.transpose(X))\n",
    "\n",
    "    # Check how well the model does on all observations by passing them through a forward pass with most up to date weights\n",
    "    XX_reshaped = np.array(XX).reshape((inputLayer_size, n))  # all observations\n",
    "    Y_reshaped = Y.reshape((outputLayer_size, n))\n",
    "    Z_hidden = np.dot(W_0, XX_reshaped) + B_0  # first hidden layer inputs\n",
    "    H = tanH(Z_hidden)  # hidden layer output (after activation)\n",
    "    Z_output = np.dot(W_1, H) + B_1  # input to output layer\n",
    "    Y_hat = sigmoid(Z_output) # Sigmoid output instead of ReLU\n",
    "    \n",
    "    # Calculate loss for entire model\n",
    "    iteration_loss = bce(Y_reshaped, Y_hat) # Here are we are using BCE\n",
    "    loss_log.append(iteration_loss)\n",
    "\n",
    "    # Development of the loss as average over observation-level losses\n",
    "    print(f'Epoch {iteration+1} with average error of {iteration_loss:.2f}.')\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just under 100 rounds, the machine was able to almost half the error size! That is a pretty good amount of progress for a simple network that we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer size for multiclass problems\n",
    "Your output layer will have one node per class for **multi-class problems**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final activation for multiclass problems\n",
    "\n",
    "If you have **single label multi-class problem** and each observation can only be 1 class (eg. classifying pictures of fish, dogs and cats), you will also want to make sure that the probability of all classes sums to 1. This is when you will want to use the **softmax** function as the final activation.\n",
    "\n",
    "If you have a **multiple label multi-class problem**, simultaneous labels are possible (eg. which products will a consumer buy). This means they no longer need to sum to 1. In this case, a **sigmoid** is fine as a final activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for multiclass problems\n",
    "For **single label multi-class problems**, you should use categorical cross-entropy or the generalized version of the equation for $ L $ above.\n",
    "\n",
    "For **multi-label multi-class problems**, you can use binary cross-entropy again because any label has its own probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat sheet for each scenario:\n",
    "\n",
    "For **regression problems**:\n",
    "- Make sure your output is 1 variable with any range\n",
    "- Output layer should be size 1\n",
    "- Output could be any number\n",
    "- Final activation function: linear or ReLU\n",
    "- Most popular loss function: MSE or MAE\n",
    "\n",
    "For **binary classification problem with target coded as a single dummy vector**:\n",
    "- Make sure your output is a single binary variable with values 0 and 1\n",
    "- Output layer should be size 1\n",
    "- Output will range from 0 to 1 and represent the probability of being class 1\n",
    "- Final activation function: sigmoid\n",
    "- Loss function: binary cross entropy\n",
    "\n",
    "For **binary classification problem with target coded as two complementary dummy:\n",
    "- Make sure your output is two binary variables representing probabilities of class 0 and class 1\n",
    "- Output layer should be size 2\n",
    "- Output nodes will range from 0 to 1 and represent the probability of being class 0 and 1 respectively\n",
    "- Final activation function: softmax\n",
    "- Loss function: binary cross entropy\n",
    "\n",
    "For **multiple possible classes but strictly 1 class per observation (single label)**:\n",
    "- Make sure your output size is equivalent to the number of classes\n",
    "- Each value for outputs must be between 0 and 1\n",
    "- Output layer should be equivalent to the number of classes\n",
    "- Output will range for each class from 0 to 1 and represent the probability of being that class\n",
    "- Final activation: softmax\n",
    "- Loss function: categorical cross entropy\n",
    "\n",
    "For **multiple possible classes and multiple classes per observation are possible (multiple labels)**:\n",
    "- Make sure your output size is equivalent to the number of classes\n",
    "- Each value for outputs must be between 0 and 1\n",
    "- Output layer should be equivalent to the number of classes\n",
    "- Output will range for each class from 0 to 1 and represent the probability of being that class\n",
    "- Final activation: sigmoid\n",
    "- Loss function: binary cross entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
